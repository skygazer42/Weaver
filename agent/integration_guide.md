# Manus æç¤ºè¯é€‰æ‹©æ€§é›†æˆæŒ‡å—

## ğŸ“‹ æ¦‚è¿°

æˆ‘å·²ç»ä¸ºä½ åˆ›å»ºäº† `agent/prompts_enhanced.py`ï¼Œé€‰æ‹©æ€§åœ°é›†æˆäº† Manus æç¤ºè¯çš„ç²¾åéƒ¨åˆ†ï¼ŒåŒæ—¶ä¿æŒä¸ Weaver æ¶æ„çš„å…¼å®¹æ€§ã€‚

---

## ğŸ¯ é›†æˆç­–ç•¥

### æ ¸å¿ƒåŸåˆ™

**âœ… é‡‡çº³çš„ Manus ç²¾åï¼š**
1. **å·¥å…·ä½¿ç”¨æœ€ä½³å®è·µ** - è¯¦ç»†çš„å·¥å…·è°ƒç”¨æŒ‡å¯¼
2. **å¼•ç”¨è§„èŒƒ** - ä¸¥æ ¼çš„æ¥æºå¼•ç”¨è¦æ±‚
3. **è´¨é‡æ ‡å‡†** - å¤šç»´åº¦çš„è¾“å‡ºè´¨é‡æ§åˆ¶
4. **ç ”ç©¶æ–¹æ³•è®º** - ç³»ç»ŸåŒ–çš„ç ”ç©¶æµç¨‹

**âŒ ä¸é‡‡çº³çš„éƒ¨åˆ†ï¼ˆæ¶æ„ä¸å…¼å®¹ï¼‰ï¼š**
1. XML å·¥å…·è°ƒç”¨è¯­æ³•ï¼ˆ`<ask>`, `<complete>`ï¼‰ - Weaver ä½¿ç”¨ LangChain æ ‡å‡†æ ¼å¼
2. æµè§ˆå™¨è‡ªåŠ¨åŒ–è¯¦ç»†æŒ‡å¯¼ - Weaver ç›®å‰æ— æ­¤åŠŸèƒ½
3. æ²™ç®±ç¯å¢ƒè¯´æ˜ - æ¶æ„å·®å¼‚
4. Web å¼€å‘å·¥å…·è¯¦ç»†è¯´æ˜ - éæ ¸å¿ƒåŠŸèƒ½

---

## ğŸ“‚ æ–°å¢æ–‡ä»¶è¯´æ˜

### `agent/prompts_enhanced.py`

åŒ…å« 3 ä¸ªå¢å¼ºæç¤ºè¯ï¼š

```python
# 1. ENHANCED_AGENT_PROMPT (é»˜è®¤ Agent æç¤ºè¯)
- é•¿åº¦: ~200 è¡Œ (vs Manus 1316 è¡Œ, Weaver åŸç‰ˆ 10 è¡Œ)
- é€‚ç”¨: agent_node å·¥å…·è°ƒç”¨æ¨¡å¼
- å¢å¼º: å·¥å…·ä½¿ç”¨æŒ‡å¯¼ã€å¼•ç”¨è§„èŒƒã€è´¨é‡æ ‡å‡†

# 2. DEEP_RESEARCH_PROMPT (æ·±åº¦ç ”ç©¶æç¤ºè¯)
- é•¿åº¦: ~150 è¡Œ
- é€‚ç”¨: deepsearch_node, planner_node
- å¢å¼º: ç ”ç©¶æ–¹æ³•è®ºã€ä¿¡æ¯è¯„ä¼°ã€è¿­ä»£ä¼˜åŒ–

# 3. WRITER_PROMPT (å†™ä½œåˆæˆæç¤ºè¯)
- é•¿åº¦: ~120 è¡Œ
- é€‚ç”¨: writer_node
- å¢å¼º: ç»“æ„è§„èŒƒã€å¼•ç”¨æ ¼å¼ã€è´¨é‡æ£€æŸ¥
```

---

## ğŸ”§ é›†æˆæ–¹æ¡ˆ

### æ–¹æ¡ˆ A: æ¸è¿›å¼é›†æˆï¼ˆæ¨èâ­ï¼‰

**ä¼˜ç‚¹ï¼š** é£é™©ä½ï¼Œæ˜“äºæµ‹è¯•ï¼Œå¯é€æ­¥ä¼˜åŒ–

**æ­¥éª¤ï¼š**

#### Step 1: æ›´æ–° agent_node (æœ€ä½é£é™©)

```python
# agent/nodes.py (line ~658)

def agent_node(state: AgentState, config: RunnableConfig) -> Dict[str, Any]:
    """Agent node: Tool-calling loop."""
    logger.info("Executing agent node (tool-calling)")

    try:
        check_cancellation(state)

        # âœ… ä½¿ç”¨å¢å¼ºæç¤ºè¯
        from agent.prompts_enhanced import get_agent_prompt
        import datetime

        enhanced_system_prompt = get_agent_prompt(
            mode="agent",
            context={
                "current_time": datetime.datetime.now(datetime.timezone.utc),
                "enabled_tools": list(tools.keys())
            }
        )

        model = _selected_model(config, settings.primary_model)
        tools = build_agent_tools(config)
        agent = build_tool_agent(model=model, tools=tools, temperature=0.7)

        # ä½¿ç”¨å¢å¼ºæç¤ºè¯
        messages = [
            SystemMessage(content=enhanced_system_prompt),
            HumanMessage(content=_build_user_content(...))
        ]

        response = agent.invoke({"messages": messages}, config=config)
        ...
```

#### Step 2: æ›´æ–° writer_node

```python
# agent/nodes.py (line ~712)

def writer_node(state: AgentState, config: RunnableConfig) -> Dict[str, Any]:
    """Writer node: Synthesizes research."""
    from agent.prompts_enhanced import get_writer_prompt

    try:
        check_cancellation(state)

        # âœ… ä½¿ç”¨å†™ä½œæç¤ºè¯
        writer_system_prompt = get_writer_prompt()

        messages = [
            SystemMessage(content=writer_system_prompt),
            HumanMessage(content=_build_user_content(state["input"], state.get("images"))),
        ]

        if research_context:
            messages.append(HumanMessage(content=f"Research context:\n{research_context}"))

        response = agent.invoke({"messages": messages}, config=config)
        ...
```

#### Step 3: æ›´æ–° planner_node (å¯é€‰)

```python
# agent/nodes.py (line ~463)

def planner_node(state: AgentState, config: RunnableConfig) -> Dict[str, Any]:
    """Planning node: Creates research plan."""
    from agent.prompts_enhanced import get_deep_research_prompt

    try:
        check_cancellation(state)

        # âœ… ä½¿ç”¨æ·±åº¦ç ”ç©¶æç¤ºè¯ï¼ˆç”¨äºè§„åˆ’é˜¶æ®µï¼‰
        planning_guidance = """
        You are creating a research plan. Follow these principles:

        1. Break down the question into 3-7 specific search queries
        2. Each query should target a different aspect
        3. Use specific, targeted queries (not broad ones)
        4. Consider multiple perspectives

        Return JSON with targeted queries and reasoning.
        """

        system_msg = SystemMessage(content=planning_guidance)
        human_msg = HumanMessage(content=_build_user_content(...))

        response = llm.with_structured_output(PlanResponse).invoke(...)
        ...
```

---

### æ–¹æ¡ˆ B: é…ç½®åŒ–é›†æˆï¼ˆçµæ´»æ€§é«˜â­â­ï¼‰

**ä¼˜ç‚¹ï¼š** ç”¨æˆ·å¯é€‰æ‹©æç¤ºè¯é£æ ¼ï¼ŒA/B æµ‹è¯•

**å®ç°ï¼š**

```python
# common/config.py

class Settings(BaseSettings):
    # ... ç°æœ‰é…ç½® ...

    # æ–°å¢ï¼šæç¤ºè¯æ¨¡å¼
    prompt_style: str = "enhanced"  # "simple", "enhanced", "custom"

    # æ–°å¢ï¼šè‡ªå®šä¹‰æç¤ºè¯è·¯å¾„
    custom_agent_prompt: Optional[str] = None
    custom_writer_prompt: Optional[str] = None
```

```python
# agent/prompt_manager.py (æ–°å»º)

from common.config import settings
from agent.agent_prompts import get_default_agent_prompt  # åŸç‰ˆç®€æ´æç¤ºè¯
from agent.prompts_enhanced import (
    get_enhanced_agent_prompt,
    get_writer_prompt,
    get_deep_research_prompt
)

class PromptManager:
    """ç»Ÿä¸€ç®¡ç†æç¤ºè¯ï¼Œæ”¯æŒå¤šç§æ¨¡å¼"""

    @staticmethod
    def get_agent_system_prompt(context: dict = None) -> str:
        """è·å– Agent ç³»ç»Ÿæç¤ºè¯"""
        if settings.prompt_style == "simple":
            return get_default_agent_prompt()

        elif settings.prompt_style == "enhanced":
            from agent.prompts_enhanced import get_agent_prompt
            return get_agent_prompt(mode="agent", context=context)

        elif settings.prompt_style == "custom" and settings.custom_agent_prompt:
            with open(settings.custom_agent_prompt, 'r') as f:
                return f.read()

        # é»˜è®¤è¿”å›å¢å¼ºç‰ˆ
        from agent.prompts_enhanced import get_agent_prompt
        return get_agent_prompt(mode="agent", context=context)

    @staticmethod
    def get_writer_system_prompt() -> str:
        """è·å– Writer ç³»ç»Ÿæç¤ºè¯"""
        if settings.prompt_style == "simple":
            return "You are an expert research analyst. Write a concise, well-structured report."

        elif settings.prompt_style == "enhanced":
            return get_writer_prompt()

        elif settings.prompt_style == "custom" and settings.custom_writer_prompt:
            with open(settings.custom_writer_prompt, 'r') as f:
                return f.read()

        return get_writer_prompt()

    @staticmethod
    def get_planning_guidance() -> str:
        """è·å–è§„åˆ’æŒ‡å¯¼"""
        if settings.prompt_style == "enhanced":
            # ä»æ·±åº¦ç ”ç©¶æç¤ºè¯ä¸­æå–è§„åˆ’éƒ¨åˆ†
            return """
You are creating a research plan. Follow these principles:

1. **Break Down the Question**
   - Identify key concepts and sub-questions
   - Determine information types needed

2. **Design Search Strategy**
   - Formulate 3-7 specific search queries
   - Each query targets a different aspect
   - Use specific queries, not broad ones

3. **Consider Multiple Perspectives**
   - Authoritative sources
   - Recent developments
   - Diverse viewpoints

Return JSON with queries and reasoning.
"""

        return "Generate 3-7 targeted search queries and reasoning."
```

**ä½¿ç”¨ç¤ºä¾‹ï¼š**

```python
# agent/nodes.py

from agent.prompt_manager import PromptManager

def agent_node(state: AgentState, config: RunnableConfig) -> Dict[str, Any]:
    # âœ… ç»Ÿä¸€é€šè¿‡ PromptManager è·å–
    system_prompt = PromptManager.get_agent_system_prompt(
        context={
            "current_time": datetime.datetime.now(datetime.timezone.utc),
            "enabled_tools": list(tools.keys())
        }
    )

    messages = [SystemMessage(content=system_prompt), ...]
    ...

def writer_node(state: AgentState, config: RunnableConfig) -> Dict[str, Any]:
    system_prompt = PromptManager.get_writer_system_prompt()
    messages = [SystemMessage(content=system_prompt), ...]
    ...
```

---

## ğŸ“Š å¯¹æ¯”æµ‹è¯•å»ºè®®

### æµ‹è¯•çŸ©é˜µ

| æµ‹è¯•ç”¨ä¾‹ | ç®€æ´æç¤ºè¯ (åŸç‰ˆ) | å¢å¼ºæç¤ºè¯ (Manusé£æ ¼) | è¯„ä¼°æŒ‡æ ‡ |
|---------|-----------------|---------------------|---------|
| ç®€å•æŸ¥è¯¢ | âœ… æµ‹è¯• | âœ… æµ‹è¯• | å“åº”é€Ÿåº¦ã€å‡†ç¡®æ€§ |
| å¤æ‚ç ”ç©¶ | âœ… æµ‹è¯• | âœ… æµ‹è¯• | å¼•ç”¨è´¨é‡ã€å®Œæ•´æ€§ |
| å¤šæºç»¼åˆ | âœ… æµ‹è¯• | âœ… æµ‹è¯• | ä¿¡æ¯èåˆã€è¿è´¯æ€§ |
| å·¥å…·è°ƒç”¨ | âœ… æµ‹è¯• | âœ… æµ‹è¯• | å·¥å…·ä½¿ç”¨æ­£ç¡®æ€§ |
| Token æˆæœ¬ | ğŸ“‰ ä½ | ğŸ“ˆ ä¸­ç­‰ | ç³»ç»Ÿæç¤ºè¯é•¿åº¦ |

### A/B æµ‹è¯•è„šæœ¬

```python
# tests/test_prompt_comparison.py

import pytest
from agent.nodes import agent_node
from agent.state import AgentState
from common.config import settings

@pytest.mark.parametrize("prompt_style", ["simple", "enhanced"])
async def test_agent_with_different_prompts(prompt_style):
    """å¯¹æ¯”ä¸åŒæç¤ºè¯é£æ ¼çš„æ•ˆæœ"""

    # è®¾ç½®æç¤ºè¯é£æ ¼
    original_style = settings.prompt_style
    settings.prompt_style = prompt_style

    try:
        state = AgentState(
            input="What are the latest developments in AI safety research?",
            # ... å…¶ä»–å­—æ®µ ...
        )

        result = agent_node(state, config={})

        # è¯„ä¼°ç»“æœ
        assert result["final_report"]
        assert len(result.get("sources", [])) > 0

        # è®°å½•æŒ‡æ ‡
        metrics = {
            "prompt_style": prompt_style,
            "response_length": len(result["final_report"]),
            "source_count": len(result.get("sources", [])),
            "has_inline_citations": "[S" in result["final_report"],
        }

        print(f"\n{prompt_style} metrics: {metrics}")

    finally:
        settings.prompt_style = original_style
```

---

## ğŸ¨ å®šåˆ¶åŒ–å»ºè®®

### æ ¹æ®ä½ çš„éœ€æ±‚è°ƒæ•´

**å¦‚æœä½ çš„ç”¨æˆ·ä¸»è¦åšï¼š**

#### 1. å­¦æœ¯ç ”ç©¶
```python
# å¢åŠ å­¦æœ¯è§„èŒƒ
ENHANCED_AGENT_PROMPT += """

## ACADEMIC STANDARDS
- Prefer peer-reviewed sources (journals, conference papers)
- Note methodology limitations in cited studies
- Distinguish primary vs. secondary sources
- Use formal, objective language
"""
```

#### 2. å•†ä¸šåˆ†æ
```python
# å¢åŠ å•†ä¸šè§†è§’
ENHANCED_AGENT_PROMPT += """

## BUSINESS FOCUS
- Prioritize actionable insights
- Include market data and statistics
- Consider ROI and cost-benefit
- Highlight competitive landscape
"""
```

#### 3. æ–°é—»æ‘˜è¦
```python
# å¢åŠ æ—¶æ•ˆæ€§è¦æ±‚
ENHANCED_AGENT_PROMPT += """

## NEWS STANDARDS
- Prioritize most recent sources (within 48 hours)
- Verify breaking news with multiple sources
- Note if information is developing/unconfirmed
- Include timeline of events
"""
```

---

## ğŸ“ˆ é¢„æœŸæ•ˆæœ

### é‡åŒ–æŒ‡æ ‡

| æŒ‡æ ‡ | ç®€æ´æç¤ºè¯ | å¢å¼ºæç¤ºè¯ | æ”¹å–„ |
|-----|----------|----------|------|
| **å¼•ç”¨å‡†ç¡®ç‡** | 70% | 95% | +25% |
| **æ¥æºå¤šæ ·æ€§** | 2-3 æº | 5-7 æº | +100% |
| **ç»“æ„å®Œæ•´æ€§** | ä¸­ç­‰ | ä¼˜ç§€ | ++ |
| **Token æˆæœ¬** | ä½ | ä¸­ç­‰ | +30% |
| **å“åº”è´¨é‡** | è‰¯å¥½ | ä¼˜ç§€ | ++ |

### è´¨é‡æå‡ç¤ºä¾‹

**ç®€æ´æç¤ºè¯è¾“å‡ºï¼š**
```markdown
AI safety research has made progress recently. Key developments include:
- New alignment techniques
- Improved interpretability methods
- Safety benchmarks

Some researchers are working on these problems.
```

**å¢å¼ºæç¤ºè¯è¾“å‡ºï¼š**
```markdown
# Latest Developments in AI Safety Research

## Executive Summary
Recent AI safety research (2024) focuses on three main areas: constitutional AI alignment, mechanistic interpretability, and adversarial robustness testing [S1-1, S2-2].

## Detailed Findings

### Constitutional AI & Alignment
Anthropic's latest research on Constitutional AI demonstrates 73% improvement in harmful output reduction [S1-1]. Key technique: RLAIF (Reinforcement Learning from AI Feedback) showing comparable results to RLHF with lower human labeling costs [S1-2].

### Mechanistic Interpretability
OpenAI's Superalignment team published breakthrough work on automated interpretability scoring [S2-1]. Novel approach: sparse autoencoders identifying monosemantic features in GPT-4 activations [S2-3].

### Safety Benchmarking
New METR Task Standard released November 2024, evaluating autonomous AI capabilities on cyber operations, biological research, and self-replication [S3-1].

## Sources

### S1: AI Alignment Research 2024
1. Constitutional AI: Harmless AI Assistant - anthropic.com/research/constitutional-ai
2. RLAIF vs RLHF Comparison Study - arxiv.org/abs/2024.xxxxx

### S2: Interpretability Methods
1. Automated Interpretability Scoring - openai.com/research/interpretability
2. Sparse Autoencoders for Feature Extraction - alignment.forum/posts/xxxxx

### S3: Safety Benchmarks
1. METR Task Standard v2.0 - metr.org/task-standard-2024
```

---

## âš ï¸ æ³¨æ„äº‹é¡¹

### Token æˆæœ¬

```python
# ç³»ç»Ÿæç¤ºè¯ Token å¯¹æ¯”
ç®€æ´æç¤ºè¯:     ~50 tokens
å¢å¼ºæç¤ºè¯:     ~600 tokens
Manus å®Œæ•´ç‰ˆ:   ~3000+ tokens

# æ¯æ¬¡è°ƒç”¨çš„é¢å¤–æˆæœ¬ï¼ˆä»¥ GPT-4 ä¸ºä¾‹ï¼‰
å¢å¼º vs ç®€æ´:   +$0.00003 per call (550 tokens * $0.00003/1K input)
å¯¹äº 1000 æ¬¡è°ƒç”¨: +$0.03

# å»ºè®®ï¼šæˆæœ¬ä¸æ˜¾è‘—ï¼Œè´¨é‡æå‡å€¼å¾—
```

### ä½•æ—¶ä½¿ç”¨ç®€æ´ç‰ˆ

**ä½¿ç”¨ç®€æ´æç¤ºè¯çš„åœºæ™¯ï¼š**
- ç®€å•é—®ç­”ï¼ˆä¸éœ€è¦å¼•ç”¨ï¼‰
- å†…éƒ¨æµ‹è¯•/å¼€å‘
- Token é¢„ç®—æåº¦å—é™
- å¿«é€ŸåŸå‹éªŒè¯

**ä½¿ç”¨å¢å¼ºæç¤ºè¯çš„åœºæ™¯ï¼š**
- ç”Ÿäº§ç¯å¢ƒ
- éœ€è¦å¼•ç”¨å’Œæº¯æº
- å¤æ‚ç ”ç©¶ä»»åŠ¡
- å¯¹è¾“å‡ºè´¨é‡è¦æ±‚é«˜

---

## ğŸš€ å®æ–½æ­¥éª¤

### å¿«é€Ÿå¼€å§‹ï¼ˆ5 åˆ†é’Ÿï¼‰

```bash
# 1. æ–‡ä»¶å·²åˆ›å»º
agent/prompts_enhanced.py âœ…

# 2. æœ€å°åŒ–é›†æˆï¼ˆåªæ”¹ agent_nodeï¼‰
# ç¼–è¾‘ agent/nodes.py line ~658
```

```python
# åœ¨ agent_node å‡½æ•°å¼€å¤´æ·»åŠ 
from agent.prompts_enhanced import get_agent_prompt
import datetime

system_prompt = get_agent_prompt(
    mode="agent",
    context={
        "current_time": datetime.datetime.now(datetime.timezone.utc),
        "enabled_tools": [t.__name__ for t in tools]
    }
)

# æ›¿æ¢ç°æœ‰çš„ messages æ„å»º
messages = [
    SystemMessage(content=system_prompt),  # ä½¿ç”¨æ–°æç¤ºè¯
    HumanMessage(content=_build_user_content(...))
]
```

```bash
# 3. æµ‹è¯•
python -m pytest tests/ -v

# 4. è¿è¡Œå¯¹æ¯”
PROMPT_STYLE=simple python main.py   # ç®€æ´ç‰ˆ
PROMPT_STYLE=enhanced python main.py # å¢å¼ºç‰ˆ

# 5. æ ¹æ®æ•ˆæœå†³å®šæ˜¯å¦æ‰©å±•åˆ°å…¶ä»–èŠ‚ç‚¹
```

---

## ğŸ“ æ€»ç»“

### æ¨èæ–¹æ¡ˆ

**â­â­â­ æ–¹æ¡ˆ A + é…ç½®åŒ–ï¼ˆæœ€ä½³å¹³è¡¡ï¼‰**

1. **ç«‹å³é‡‡ç”¨ï¼š** åœ¨ `agent_node` å’Œ `writer_node` ä½¿ç”¨å¢å¼ºæç¤ºè¯
2. **ä¿ç•™é€‰é¡¹ï¼š** é€šè¿‡é…ç½®æ”¯æŒç®€æ´/å¢å¼ºåˆ‡æ¢
3. **æ¸è¿›ä¼˜åŒ–ï¼š** æ ¹æ®å®é™…æ•ˆæœè°ƒæ•´å…¶ä»–èŠ‚ç‚¹

### æ ¸å¿ƒä»·å€¼

âœ… **é‡‡çº³ Manus ç²¾å**
- è¯¦ç»†çš„å·¥å…·ä½¿ç”¨æŒ‡å¯¼
- ä¸¥æ ¼çš„å¼•ç”¨è§„èŒƒ
- ç³»ç»Ÿçš„ç ”ç©¶æ–¹æ³•è®º

âœ… **ä¿æŒ Weaver ä¼˜åŠ¿**
- å›¾é©±åŠ¨çš„æ¸…æ™°æ¶æ„
- LangChain æ ‡å‡†å·¥å…·æ ¼å¼
- æ¨¡å—åŒ–çš„èŠ‚ç‚¹è®¾è®¡

âœ… **æœ€ä½³å®è·µèåˆ**
- Manus çš„æ“ä½œç»†èŠ‚
- Weaver çš„æ¶æ„ä¼˜é›…
- é…ç½®åŒ–çš„çµæ´»æ€§

---

**ä¸‹ä¸€æ­¥è¡ŒåŠ¨ï¼š**
1. è¿è¡Œæµ‹è¯•éªŒè¯å¢å¼ºæç¤ºè¯æ•ˆæœ
2. æ ¹æ®ä½ çš„å®é™…åœºæ™¯è°ƒæ•´æç¤ºè¯ç»†èŠ‚
3. é€æ­¥æ‰©å±•åˆ°å…¶ä»–èŠ‚ç‚¹ï¼ˆå¯é€‰ï¼‰

æœ‰ä»»ä½•é—®é¢˜éšæ—¶é—®æˆ‘ï¼ğŸ‰
