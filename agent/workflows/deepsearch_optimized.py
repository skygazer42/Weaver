"""
Optimized DeepSearch implementation with enhanced features.

Key improvements:
1. URL deduplication mechanism
2. Detailed performance logging
3. Enhanced error handling
4. Better cancellation support
5. OOP encapsulation (optional)
6. Tree-based exploration (new)
7. Multi-model support (new)

Based on: deep_search-dev reference implementation
"""

import ast
import asyncio
import json
import logging
import re
import textwrap
import time
import traceback
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

from langchain_core.messages import AIMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

from common.cancellation import check_cancellation as _check_cancel_token
from common.config import settings
from prompts.templates.deepsearch import (
    final_summary_prompt,
    formulate_query_prompt,
    related_url_prompt,
    summary_crawl_prompt,
    summary_text_prompt,
)
from tools.crawl.crawler import crawl_urls
from tools.search.search import tavily_search

# Import tree-based research components
from agent.workflows.research_tree import TreeExplorer, ResearchTree

# Import knowledge gap analysis
from agent.workflows.knowledge_gap import KnowledgeGapAnalyzer

logger = logging.getLogger(__name__)


def _chat_model(model: str, temperature: float) -> ChatOpenAI:
    """Build a ChatOpenAI client that honors OpenAI/Azure/base_url overrides."""
    params: Dict[str, Any] = {
        "model": model,
        "temperature": temperature,
        "api_key": settings.openai_api_key,
        "timeout": settings.openai_timeout or None,
    }

    if settings.use_azure:
        params.update(
            {
                "azure_endpoint": settings.azure_endpoint or None,
                "azure_deployment": model,
                "api_version": settings.azure_api_version or None,
                "api_key": settings.azure_api_key or settings.openai_api_key,
            }
        )
    elif settings.openai_base_url:
        params["base_url"] = settings.openai_base_url

    if settings.openai_extra_body:
        try:
            params["extra_body"] = json.loads(settings.openai_extra_body)
        except json.JSONDecodeError:
            logger.warning("Invalid JSON in openai_extra_body; ignoring.")

    return ChatOpenAI(**params)


def _check_cancel(state: Dict[str, Any]) -> None:
    """Respect cancellation flags/tokens."""
    if state.get("is_cancelled"):
        raise asyncio.CancelledError("Task was cancelled (flag)")
    token_id = state.get("cancel_token_id")
    if token_id:
        _check_cancel_token(token_id)


def _selected_model(config: Dict[str, Any], fallback: str) -> str:
    cfg = config.get("configurable") or {}
    if isinstance(cfg, dict):
        val = cfg.get("model")
        if isinstance(val, str) and val.strip():
            return val.strip()
    return fallback


def _selected_reasoning_model(config: Dict[str, Any], fallback: str) -> str:
    cfg = config.get("configurable") or {}
    if isinstance(cfg, dict):
        val = cfg.get("reasoning_model")
        if isinstance(val, str) and val.strip():
            return val.strip()
    return fallback


def _model_for_task(task_type: str, config: Dict[str, Any]) -> str:
    """
    Get model name for a specific task type using the ModelRouter.

    Args:
        task_type: One of: planning, query_gen, research, critique, synthesis, writing
        config: RunnableConfig dict with optional overrides
    """
    try:
        from agent.core.multi_model import TaskType, get_model_router

        tt = TaskType(task_type)
        router = get_model_router()
        return router.get_model_name(tt, config)
    except Exception:
        # Fallback to legacy behavior
        if task_type in ("planning", "query_gen", "critique", "gap_analysis"):
            return _selected_reasoning_model(config, settings.reasoning_model)
        return _selected_model(config, settings.primary_model)


def _parse_list_output(text: str) -> List[str]:
    """Parse python-list-like output into a string list."""
    if not text:
        return []
    fenced = re.findall(r"```(?:python)?(.*?)```", text, flags=re.S | re.I)
    if fenced:
        text = fenced[-1]
    start = text.find("[")
    end = text.rfind("]")
    if start != -1 and end > start:
        text = text[start : end + 1]
    try:
        data = ast.literal_eval(text)
        if isinstance(data, list):
            return [str(x).strip() for x in data if isinstance(x, (str, int, float))]
    except Exception:
        pass
    # Fallback: split by newline
    return [line.strip() for line in text.splitlines() if line.strip()]


def _format_results(results: List[Dict[str, Any]]) -> str:
    """Format search results for prompt consumption."""
    blocks: List[str] = []
    for idx, r in enumerate(results, 1):
        blocks.append(
            textwrap.dedent(
                f"""\
                [{idx}]
                æ ‡é¢˜: {r.get("title") or "N/A"}
                æ—¥æœŸ: {r.get("published_date") or "unknown"}
                è¯„åˆ†: {r.get("score", 0)}
                é“¾æ¥: {r.get("url") or ""}
                æ‘˜è¦: {r.get("summary") or r.get("snippet") or ""}
                åŸæ–‡: {(r.get("raw_excerpt") or "")[:500]}
                """
            ).strip()
        )
    return "\n\n".join(blocks)


def _generate_queries(
    llm: ChatOpenAI,
    topic: str,
    have_query: List[str],
    summary_notes: List[str],
    query_num: int,
    config: Dict[str, Any],
    missing_topics: Optional[List[str]] = None,
) -> List[str]:
    """Generate new search queries based on topic, existing knowledge, and knowledge gaps.

    If missing_topics is provided (from gap analysis), prioritizes those areas.
    """
    # If we have missing topics from gap analysis, incorporate them
    enhanced_topic = topic
    if missing_topics:
        gap_hint = f"\n\næ³¨æ„ï¼šä»¥ä¸‹æ–¹é¢ä¿¡æ¯ä»ç„¶ä¸è¶³ï¼Œè¯·ä¼˜å…ˆè¦†ç›–ï¼š{', '.join(missing_topics[:3])}"
        enhanced_topic = topic + gap_hint

    prompt = ChatPromptTemplate.from_messages([("user", formulate_query_prompt)])
    msg = prompt.format_messages(
        topic=enhanced_topic,
        have_query=", ".join(have_query) or "[]",
        summary_search="\n\n".join(summary_notes) or "æš‚æ— ",
        query_num=query_num,
    )
    response = llm.invoke(msg, config=config)
    content = getattr(response, "content", "") or ""
    queries = _parse_list_output(content)
    # Deduplicate and trim
    seen = set(q.lower() for q in have_query)
    clean: List[str] = []
    for q in queries:
        if not q:
            continue
        q_norm = q.strip()
        if not q_norm or q_norm.lower() in seen:
            continue
        seen.add(q_norm.lower())
        clean.append(q_norm)
        if len(clean) >= query_num:
            break
    return clean


def _pick_relevant_urls(
    llm: ChatOpenAI,
    topic: str,
    summary_notes: List[str],
    results: List[Dict[str, Any]],
    max_urls: int,
    config: Dict[str, Any],
    selected_urls: List[str],  # æ–°å¢ï¼šå·²é€‰æ‹©çš„ URL
) -> List[str]:
    """Pick relevant URLs from search results, excluding already selected ones."""
    if not results:
        return []

    # è¿‡æ»¤å·²é€‰æ‹©çš„ URL
    available_results = [r for r in results if r.get("url") and r.get("url") not in selected_urls]

    if not available_results:
        logger.info("æ‰€æœ‰ URL éƒ½å·²è¢«é€‰æ‹©è¿‡ï¼Œæ— æ–° URL å¯é€‰")
        return []

    formatted = _format_results(available_results)
    prompt = ChatPromptTemplate.from_messages([("user", related_url_prompt)])
    msg = prompt.format_messages(
        topic=topic,
        summary_search="\n\n".join(summary_notes) or "æš‚æ— ",
        text=formatted,
    )
    response = llm.invoke(msg, config=config)
    urls = _parse_list_output(getattr(response, "content", "") or "")

    # Fallback: top scores
    if not urls:
        sorted_results = sorted(available_results, key=lambda r: r.get("score", 0), reverse=True)
        urls = [r.get("url") for r in sorted_results if r.get("url")]

    # Clamp
    deduped: List[str] = []
    seen = set()
    for u in urls:
        if not isinstance(u, str):
            continue
        u = u.strip()
        if not u or u in seen or u in selected_urls:
            continue
        seen.add(u)
        deduped.append(u)
        if len(deduped) >= max_urls:
            break
    return deduped


def _summarize_new_knowledge(
    llm: ChatOpenAI,
    topic: str,
    summary_notes: List[str],
    chosen_results: List[Dict[str, Any]],
    config: Dict[str, Any],
) -> Tuple[bool, str]:
    """Summarize new knowledge and judge if information is sufficient."""
    if not chosen_results:
        return False, ""

    prompt = ChatPromptTemplate.from_messages([("user", summary_crawl_prompt)])
    msg = prompt.format_messages(
        summary_search="\n\n".join(summary_notes) or "æš‚æ— ",
        crawl_res=_format_results(chosen_results),
        topic=topic,
    )
    response = llm.invoke(msg, config=config)
    content = getattr(response, "content", "") or ""
    lowered = content.lower()
    enough = "å›ç­”" in lowered and "yes" in lowered.split("å›ç­”", 1)[-1]

    # Extract summary after "æ€»ç»“:" if present
    summary_text = ""
    if "æ€»ç»“" in content:
        summary_text = content.split("æ€»ç»“", 1)[-1].strip(":ï¼š \n")
    if not summary_text:
        summary_text = content
    return enough, summary_text.strip()


def _final_report(
    llm: ChatOpenAI, topic: str, summary_notes: List[str], config: Dict[str, Any]
) -> str:
    """Generate final report based on all summaries."""
    prompt = ChatPromptTemplate.from_messages([("user", final_summary_prompt)])
    msg = prompt.format_messages(
        topic=topic,
        summary_search="\n\n".join(summary_notes) or "æš‚æ— ",
    )
    response = llm.invoke(msg, config=config)
    return getattr(response, "content", "") or summary_text_prompt


def _hydrate_with_crawler(results: List[Dict[str, Any]]) -> None:
    """Enrich results in-place with crawled content when Tavily lacks body text."""
    if not settings.deepsearch_enable_crawler or not results:
        return

    # Pick URLs that need content
    targets = []
    for r in results:
        body = r.get("raw_excerpt") or r.get("summary") or ""
        if len(body) < 200 and r.get("url"):
            targets.append(r["url"])
    if not targets:
        return

    crawled = {item["url"]: item for item in crawl_urls(targets)}
    for r in results:
        url = r.get("url")
        if not url or url not in crawled:
            continue
        content = crawled[url].get("content") or ""
        if content:
            r["raw_excerpt"] = content[:1200]
            if not r.get("summary"):
                r["summary"] = content[:400]


def _safe_filename(name: str) -> str:
    return re.sub(r'[\/\\:\*\?"<>\|]', "_", name)[:80]


def _save_deepsearch_data(
    topic: str,
    have_query: List[str],
    summary_notes: List[str],
    search_runs: List[Dict[str, Any]],
    final_report: str,
    epoch: int,
) -> str:
    """Persist deepsearch run data if enabled."""
    if not settings.deepsearch_save_data:
        return ""

    try:
        save_dir = Path(settings.deepsearch_save_dir)
        save_dir.mkdir(parents=True, exist_ok=True)
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        fname = f"{_safe_filename(topic)}_{ts}.json"
        path = save_dir / fname
        data = {
            "topic": topic,
            "queries": have_query,
            "summaries": summary_notes,
            "search_runs": search_runs,
            "final_report": final_report,
            "epoch": epoch,
            "mode": "deepsearch_optimized",
        }
        path.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding="utf-8")
        logger.info(f"[deepsearch] saved run data -> {path}")
        return str(path)
    except Exception as e:
        logger.warning(f"[deepsearch] failed to save data: {e}")
        return ""


def run_deepsearch_optimized(state: Dict[str, Any], config: Dict[str, Any]) -> Dict[str, Any]:
    """
    Optimized iterative deep-search pipeline.

    Improvements:
    1. URL deduplication to avoid repeated crawling
    2. Detailed performance logging for each step
    3. Enhanced error handling (single epoch failure doesn't break flow)
    4. Better cancellation support
    5. Maintains all_searched_urls and selected_urls
    """
    topic = state.get("input", "")
    _check_cancel(state)

    max_epochs = int(getattr(settings, "deepsearch_max_epochs", 3))
    query_num = int(getattr(settings, "deepsearch_query_num", 5))
    per_query_results = int(getattr(settings, "deepsearch_results_per_query", 5))
    top_urls = max(3, min(5, per_query_results))

    # Use multi-model routing for different task types
    planning_model = _model_for_task("planning", config)
    research_model = _model_for_task("research", config)
    writing_model = _model_for_task("writing", config)

    planner_llm = _chat_model(planning_model, temperature=0.8)
    critic_llm = _chat_model(research_model, temperature=0.2)
    writer_llm = _chat_model(writing_model, temperature=0.5)

    have_query: List[str] = []
    summary_notes: List[str] = []
    search_runs: List[Dict[str, Any]] = []

    # âœ¨ æ–°å¢ï¼šURL å»é‡æœºåˆ¶
    all_searched_urls: List[str] = []  # æ‰€æœ‰æœç´¢åˆ°çš„ URL
    selected_urls: List[str] = []  # å·²çˆ¬å–çš„ URL

    logger.info(f"[deepsearch] topic='{topic}' epochs={max_epochs}")
    logger.info(f"[deepsearch] å¼€å§‹ä¼˜åŒ–ç‰ˆæ·±åº¦æœç´¢")

    start_ts = time.time()

    try:
        for epoch in range(max_epochs):
            try:
                _check_cancel(state)
                epoch_start = time.time()
                logger.info(f"[deepsearch] ===== Epoch {epoch + 1}/{max_epochs} =====")

                # â±ï¸ Step 1: ç”ŸæˆæŸ¥è¯¢ (åˆ©ç”¨çŸ¥è¯†ç©ºç™½åˆ†æç»“æœ)
                query_start = time.time()
                missing_topics = state.get("missing_topics", []) if epoch > 0 else []
                queries = _generate_queries(
                    planner_llm, topic, have_query, summary_notes, query_num, config,
                    missing_topics=missing_topics,
                )
                if epoch == 0 and topic not in queries:
                    queries.append(topic)
                if not queries:
                    queries = [topic]
                have_query.extend(q for q in queries if q not in have_query)
                logger.info(
                    f"[deepsearch] Epoch {epoch + 1}: ç”Ÿæˆ {len(queries)} ä¸ªæŸ¥è¯¢"
                    f" | è€—æ—¶ {time.time() - query_start:.2f}s"
                )
                logger.debug(f"[deepsearch] æŸ¥è¯¢åˆ—è¡¨: {queries}")

                # â±ï¸ Step 2: å¹¶è¡Œæœç´¢
                search_start = time.time()
                combined_results: List[Dict[str, Any]] = []
                for q in queries:
                    _check_cancel(state)
                    results = tavily_search.invoke(
                        {"query": q, "max_results": per_query_results},
                        config=config,
                    )
                    combined_results.extend(results)
                    search_runs.append(
                        {
                            "query": q,
                            "results": results,
                            "timestamp": datetime.now().isoformat(),
                        }
                    )

                    # ğŸ“ è®°å½•æ‰€æœ‰æœç´¢åˆ°çš„ URLï¼ˆå»é‡ï¼‰
                    for r in results:
                        url = r.get("url")
                        if url and url not in all_searched_urls:
                            all_searched_urls.append(url)

                logger.info(
                    f"[deepsearch] Epoch {epoch + 1}: æœç´¢åˆ° {len(combined_results)} ä¸ªç»“æœ"
                    f" | ç´¯è®¡ URL: {len(all_searched_urls)}"
                    f" | è€—æ—¶ {time.time() - search_start:.2f}s"
                )

                if not combined_results:
                    logger.info(f"[deepsearch] Epoch {epoch + 1}: æ— æœç´¢ç»“æœï¼Œè·³è¿‡æœ¬è½®")
                    continue

                # â±ï¸ Step 3: æŒ‘é€‰æœ€ç›¸å…³çš„ URLï¼ˆæ’é™¤å·²é€‰æ‹©çš„ï¼‰
                pick_start = time.time()
                chosen_urls = _pick_relevant_urls(
                    critic_llm,
                    topic,
                    summary_notes,
                    combined_results,
                    top_urls,
                    config,
                    selected_urls,  # âœ¨ ä¼ å…¥å·²é€‰æ‹©çš„ URL
                )

                if not chosen_urls:
                    logger.warning(
                        f"[deepsearch] Epoch {epoch + 1}: æ— æ–° URL å¯é€‰ï¼ˆå·²å…¨éƒ¨é€‰æ‹©è¿‡ï¼‰ï¼Œè·³è¿‡æœ¬è½®"
                    )
                    continue

                # æ›´æ–°å·²é€‰æ‹©çš„ URL åˆ—è¡¨
                selected_urls.extend(chosen_urls)

                chosen_results = [r for r in combined_results if r.get("url") in set(chosen_urls)]
                if not chosen_results:
                    chosen_results = sorted(
                        combined_results, key=lambda r: r.get("score", 0), reverse=True
                    )[:top_urls]

                logger.info(
                    f"[deepsearch] Epoch {epoch + 1}: é€‰æ‹© {len(chosen_urls)} ä¸ª URL"
                    f" | å·²é€‰æ€»æ•°: {len(selected_urls)}"
                    f" | è€—æ—¶ {time.time() - pick_start:.2f}s"
                )

                # â±ï¸ Step 4: çˆ¬è™«è¡¥å……å†…å®¹ï¼ˆå¯é€‰ï¼‰
                if settings.deepsearch_enable_crawler:
                    crawl_start = time.time()
                    _hydrate_with_crawler(chosen_results)
                    logger.info(
                        f"[deepsearch] Epoch {epoch + 1}: çˆ¬è™«å¢å¼ºå®Œæˆ"
                        f" | è€—æ—¶ {time.time() - crawl_start:.2f}s"
                    )

                # â±ï¸ Step 5: æ‘˜è¦æ–°çŸ¥è¯† + åˆ¤æ–­æ˜¯å¦è¶³å¤Ÿ
                summary_start = time.time()
                enough, summary_text = _summarize_new_knowledge(
                    critic_llm, topic, summary_notes, chosen_results, config
                )
                if summary_text:
                    summary_notes.append(summary_text)

                logger.info(
                    f"[deepsearch] Epoch {epoch + 1}: æ‘˜è¦å®Œæˆ"
                    f" | è¶³å¤Ÿ: {enough}"
                    f" | æ‘˜è¦é•¿åº¦: {len(summary_text)}"
                    f" | è€—æ—¶ {time.time() - summary_start:.2f}s"
                )

                # â±ï¸ Step 5.5: çŸ¥è¯†ç©ºç™½åˆ†æ (å¯é€‰)
                use_gap_analysis = getattr(settings, "deepsearch_use_gap_analysis", True)
                if use_gap_analysis and not enough and epoch < max_epochs - 1:
                    gap_start = time.time()
                    try:
                        gap_model = _model_for_task("gap_analysis", config)
                        gap_llm = _chat_model(gap_model, temperature=0.3)
                        gap_analyzer = KnowledgeGapAnalyzer(gap_llm, config, coverage_threshold=0.8)

                        # Analyze current knowledge state
                        collected_knowledge = "\n\n".join(summary_notes)
                        gap_result = gap_analyzer.analyze(topic, have_query, collected_knowledge)

                        logger.info(
                            f"[deepsearch] Epoch {epoch + 1}: çŸ¥è¯†ç©ºç™½åˆ†æå®Œæˆ"
                            f" | è¦†ç›–ç‡: {gap_result.overall_coverage:.2f}"
                            f" | ç©ºç™½æ•°: {len(gap_result.gaps)}"
                            f" | è€—æ—¶ {time.time() - gap_start:.2f}s"
                        )

                        # Use gap analysis to determine if we can stop early
                        if gap_analyzer.is_research_sufficient(gap_result):
                            logger.info(f"[deepsearch] Epoch {epoch + 1}: çŸ¥è¯†ç©ºç™½åˆ†æåˆ¤å®šä¿¡æ¯è¶³å¤Ÿ")
                            enough = True

                        # Get high-priority aspects for next round's query generation
                        high_priority_aspects = gap_analyzer.get_high_priority_aspects(gap_result)
                        if high_priority_aspects:
                            logger.info(
                                f"[deepsearch] é«˜ä¼˜å…ˆçº§ç©ºç™½: {', '.join(high_priority_aspects[:3])}"
                            )
                            # Store for use in next epoch's query generation
                            state["missing_topics"] = high_priority_aspects

                    except Exception as e:
                        logger.warning(f"[deepsearch] çŸ¥è¯†ç©ºç™½åˆ†æå¤±è´¥ï¼Œç»§ç»­å¸¸è§„æµç¨‹: {e}")

                epoch_duration = time.time() - epoch_start
                logger.info(f"[deepsearch] Epoch {epoch + 1}: æ€»è€—æ—¶ {epoch_duration:.2f}s")

                # å¦‚æœä¿¡æ¯è¶³å¤Ÿï¼Œæå‰ç»“æŸ
                if enough:
                    logger.info(f"[deepsearch] Epoch {epoch + 1}: ä¿¡æ¯å·²è¶³å¤Ÿï¼Œæå‰ç»“æŸ")
                    break

            except asyncio.CancelledError:
                raise  # ç»§ç»­å‘ä¸ŠæŠ›å‡º
            except Exception as e:
                logger.error(f"[deepsearch] Epoch {epoch + 1} å¤±è´¥: {str(e)}", exc_info=True)
                logger.error(traceback.format_exc())
                logger.info(f"[deepsearch] ç»§ç»­ä¸‹ä¸€è½®æœç´¢...")
                continue  # å•è½®å¤±è´¥ä¸å½±å“æ•´ä½“æµç¨‹

        # â±ï¸ Step 6: ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        report_start = time.time()
        final_report = (
            _final_report(writer_llm, topic, summary_notes, config)
            if summary_notes
            else summary_text_prompt
        )
        logger.info(
            f"[deepsearch] æœ€ç»ˆæŠ¥å‘Šç”Ÿæˆå®Œæˆ"
            f" | å­—æ•°: {len(final_report)}"
            f" | è€—æ—¶ {time.time() - report_start:.2f}s"
        )

        elapsed = time.time() - start_ts
        logger.info(
            f"[deepsearch] ===== å®Œæˆ ====="
            f"\n  æ€»è€—æ—¶: {elapsed:.2f}s"
            f"\n  æ€»è½®æ¬¡: {epoch + 1}"
            f"\n  æ€»æŸ¥è¯¢: {len(have_query)}"
            f"\n  æ€» URL: {len(all_searched_urls)}"
            f"\n  å·²çˆ¬å–: {len(selected_urls)}"
            f"\n  æ‘˜è¦æ•°: {len(summary_notes)}"
        )

        # ä¿å­˜æ•°æ®
        save_path = _save_deepsearch_data(
            topic,
            have_query,
            summary_notes,
            search_runs,
            final_report,
            epoch=epoch + 1,
        )

        messages = [AIMessage(content=final_report)]
        if save_path:
            messages.append(AIMessage(content=f"(æ•°æ®å·²ä¿å­˜: {save_path})"))

        return {
            "research_plan": have_query,
            "scraped_content": search_runs,
            "draft_report": final_report,
            "final_report": final_report,
            "messages": messages,
            "is_complete": False,
        }

    except asyncio.CancelledError:
        logger.warning("[deepsearch] æ”¶åˆ°å–æ¶ˆä¿¡å·ï¼Œåœæ­¢ä»»åŠ¡")
        return {
            "is_cancelled": True,
            "is_complete": True,
            "errors": ["DeepSearch was cancelled"],
            "final_report": "ä»»åŠ¡å·²è¢«å–æ¶ˆ",
        }


def run_deepsearch_tree(state: Dict[str, Any], config: Dict[str, Any]) -> Dict[str, Any]:
    """
    Tree-based deep search pipeline.

    Uses hierarchical topic decomposition and parallel branch exploration
    for more comprehensive research coverage.

    Inspired by GPT Researcher's tree exploration approach.
    """
    topic = state.get("input", "")
    _check_cancel(state)

    # Use multi-model routing for different task types
    planning_model = _model_for_task("planning", config)
    research_model = _model_for_task("research", config)
    writing_model = _model_for_task("writing", config)

    planner_llm = _chat_model(planning_model, temperature=0.8)
    critic_llm = _chat_model(research_model, temperature=0.2)
    writer_llm = _chat_model(writing_model, temperature=0.5)

    max_depth = int(getattr(settings, "tree_max_depth", 2))
    max_branches = int(getattr(settings, "tree_max_branches", 4))
    queries_per_branch = int(getattr(settings, "tree_queries_per_branch", 3))
    per_query_results = int(getattr(settings, "deepsearch_results_per_query", 5))
    parallel_branches = int(getattr(settings, "tree_parallel_branches", 3))

    logger.info(
        f"[deepsearch-tree] Starting tree exploration: topic='{topic}' "
        f"depth={max_depth} branches={max_branches} parallel={parallel_branches}"
    )

    start_ts = time.time()

    try:
        # Create tree explorer
        explorer = TreeExplorer(
            planner_llm=planner_llm,
            researcher_llm=critic_llm,
            writer_llm=writer_llm,
            search_func=tavily_search.invoke,
            config=config,
            max_depth=max_depth,
            max_branches=max_branches,
            queries_per_branch=queries_per_branch,
        )

        # Run tree exploration (use async if parallel_branches > 0)
        if parallel_branches > 0:
            # Use async parallel exploration
            try:
                loop = asyncio.get_event_loop()
                if loop.is_running():
                    # If already in async context, use run_in_executor
                    import concurrent.futures
                    with concurrent.futures.ThreadPoolExecutor() as executor:
                        future = executor.submit(
                            lambda: asyncio.run(explorer.run_async(topic, state, decompose_root=True))
                        )
                        tree = future.result()
                else:
                    tree = loop.run_until_complete(explorer.run_async(topic, state, decompose_root=True))
            except RuntimeError:
                # No event loop, create one
                tree = asyncio.run(explorer.run_async(topic, state, decompose_root=True))
            logger.info(f"[deepsearch-tree] Used async parallel exploration")
        else:
            tree = explorer.run(topic, state, decompose_root=True)

        # Get merged summary from all branches
        merged_summary = explorer.get_final_summary()
        all_sources = explorer.get_all_sources()
        all_findings = explorer.get_all_findings()

        # Generate final report using the comprehensive summary
        summary_notes = [merged_summary] if merged_summary else []
        final_report = (
            _final_report(writer_llm, topic, summary_notes, config)
            if summary_notes
            else summary_text_prompt
        )

        elapsed = time.time() - start_ts
        logger.info(
            f"[deepsearch-tree] ===== Completed =====\n"
            f"  Total time: {elapsed:.2f}s\n"
            f"  Tree nodes: {len(tree.nodes)}\n"
            f"  Total sources: {len(all_sources)}\n"
            f"  Report length: {len(final_report)} chars"
        )

        # Collect queries from all nodes
        have_query = []
        search_runs = []
        for node in tree.nodes.values():
            have_query.extend(node.queries)
            for finding in node.findings:
                search_runs.append({
                    "query": finding.get("query", ""),
                    "results": [finding.get("result", {})],
                    "timestamp": finding.get("timestamp", ""),
                    "branch_id": node.id,
                    "branch_topic": node.topic,
                })

        # Save data
        save_path = _save_deepsearch_data(
            topic, have_query, summary_notes, search_runs, final_report, epoch=1,
        )

        messages = [AIMessage(content=final_report)]
        if save_path:
            messages.append(AIMessage(content=f"(æ•°æ®å·²ä¿å­˜: {save_path})"))

        return {
            "research_plan": have_query,
            "scraped_content": search_runs,
            "draft_report": final_report,
            "final_report": final_report,
            "messages": messages,
            "research_tree": tree.to_dict(),
            "is_complete": False,
        }

    except asyncio.CancelledError:
        logger.warning("[deepsearch-tree] æ”¶åˆ°å–æ¶ˆä¿¡å·ï¼Œåœæ­¢ä»»åŠ¡")
        return {
            "is_cancelled": True,
            "is_complete": True,
            "errors": ["DeepSearch was cancelled"],
            "final_report": "ä»»åŠ¡å·²è¢«å–æ¶ˆ",
        }
    except Exception as e:
        logger.error(f"[deepsearch-tree] Failed: {e}", exc_info=True)
        # Fallback to linear mode
        logger.info("[deepsearch-tree] Falling back to linear deepsearch...")
        return run_deepsearch_optimized(state, config)


def run_deepsearch_auto(state: Dict[str, Any], config: Dict[str, Any]) -> Dict[str, Any]:
    """
    Auto-select between tree and linear deep search based on settings.

    Uses tree-based exploration if enabled in settings, otherwise falls back
    to the optimized linear approach.
    """
    use_tree = getattr(settings, "tree_exploration_enabled", True)

    if use_tree:
        logger.info("[deepsearch] Using tree-based exploration mode")
        return run_deepsearch_tree(state, config)
    else:
        logger.info("[deepsearch] Using linear exploration mode")
        return run_deepsearch_optimized(state, config)
