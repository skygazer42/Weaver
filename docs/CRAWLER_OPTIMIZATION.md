# Crawler ä¼˜åŒ–æ–¹æ¡ˆ

## ğŸ“Š å¯¹æ¯”åˆ†æ

### å½“å‰å®ç° vs å‚è€ƒé¡¹ç›® vs ä¼˜åŒ–ç‰ˆæœ¬

| ç»´åº¦ | å½“å‰ç‰ˆæœ¬<br/>(crawler.py) | å‚è€ƒé¡¹ç›®<br/>(crawler_api.py) | ä¼˜åŒ–ç‰ˆæœ¬<br/>(crawler_optimized.py) |
|------|---------|---------|---------|
| **æ ¸å¿ƒæŠ€æœ¯** | urllib | Playwright | Playwright |
| **JSæ¸²æŸ“** | âŒ | âœ… | âœ… |
| **å¹¶å‘çˆ¬å–** | âŒ (é¡ºåº) | âœ… (asyncio.gather) | âœ… (asyncio.gather + Semaphore) |
| **æ–‡æœ¬æå–** | âŒ (Regex) | âœ… (page.inner_text) | âœ… (page.inner_text) |
| **æµè§ˆå™¨ç®¡ç†** | N/A | âœ… (Context Manager) | âœ… (Context Manager + Singleton) |
| **é”™è¯¯å¤„ç†** | â­â­ | â­â­â­â­ | â­â­â­â­â­ |
| **æ€§èƒ½** | æ…¢ (é¡ºåº) | å¿« (å¹¶å‘) | æœ€å¿« (å¹¶å‘ + å¹¶å‘é™åˆ¶) |
| **é…ç½®åŒ–** | âŒ | âš ï¸ (ç¡¬ç¼–ç ) | âœ… (settingsé›†æˆ) |
| **å‘åå…¼å®¹** | N/A | âŒ | âœ… (åŒæ­¥åŒ…è£…å™¨) |
| **é™çº§æ–¹æ¡ˆ** | N/A | âŒ | âœ… (fallback to urllib) |

## ğŸ”§ å½“å‰å®ç°çš„é—®é¢˜

### 1. **æ—  JavaScript æ¸²æŸ“æ”¯æŒ** â­â­â­â­â­

**é—®é¢˜**ï¼šç°ä»£ç½‘ç«™å¤§é‡ä½¿ç”¨ JavaScript åŠ¨æ€åŠ è½½å†…å®¹ï¼Œurllib æ— æ³•è·å–è¿™äº›å†…å®¹ã€‚

**ç¤ºä¾‹åœºæ™¯**ï¼š
- SPA (Single Page Application) åº”ç”¨
- åŠ¨æ€åŠ è½½çš„æ–°é—»å†…å®¹
- React/Vue æ„å»ºçš„ç°ä»£ç½‘ç«™

**å½±å“**ï¼š
- çˆ¬å–çš„å†…å®¹ä¸å®Œæ•´æˆ–ä¸ºç©º
- é”™è¿‡å…³é”®ä¿¡æ¯
- é™ä½ DeepSearch çš„è´¨é‡

---

### 2. **é¡ºåºçˆ¬å–æ€§èƒ½å·®** â­â­â­â­â­

**é—®é¢˜**ï¼šå½“å‰å®ç°æ˜¯é¡ºåºçˆ¬å–ï¼Œæ¯ä¸ª URL å¿…é¡»ç­‰å¾…å‰ä¸€ä¸ªå®Œæˆã€‚

**ä»£ç ç‰‡æ®µ** (crawler.py:54-61):
```python
def crawl_urls(urls: List[str], timeout: int = 10) -> List[Dict[str, str]]:
    results: List[Dict[str, str]] = []
    for u in urls:  # é¡ºåºå¾ªç¯
        if not u:
            continue
        results.append(crawl_url(u, timeout=timeout))  # é€ä¸ªç­‰å¾…
    return results
```

**æ€§èƒ½å¯¹æ¯”**ï¼š
```
åœºæ™¯ï¼šçˆ¬å– 5 ä¸ª URLï¼Œæ¯ä¸ªè€—æ—¶ 3 ç§’

å½“å‰å®ç°ï¼ˆé¡ºåºï¼‰ï¼š
  URL1: 0s -> 3s
  URL2: 3s -> 6s
  URL3: 6s -> 9s
  URL4: 9s -> 12s
  URL5: 12s -> 15s
  æ€»è€—æ—¶: 15s âŒ

ä¼˜åŒ–ç‰ˆæœ¬ï¼ˆå¹¶å‘ï¼‰ï¼š
  URL1: 0s -> 3s â”
  URL2: 0s -> 3s â”œâ”€ å¹¶å‘æ‰§è¡Œ
  URL3: 0s -> 3s â”‚
  URL4: 0s -> 3s â”‚
  URL5: 0s -> 3s â”˜
  æ€»è€—æ—¶: 3s âœ… (æé€Ÿ 5x)
```

---

### 3. **ç®€é™‹çš„ HTML è§£æ** â­â­â­â­

**é—®é¢˜**ï¼šä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å¤„ç† HTML æ˜¯ä¸å¯é çš„ã€‚

**ä»£ç ç‰‡æ®µ** (crawler.py:23-34):
```python
def _strip_html(html: str) -> str:
    # Remove scripts/styles
    html = re.sub(r"<script.*?>.*?</script>", "", html, flags=re.S | re.I)
    html = re.sub(r"<style.*?>.*?</style>", "", html, flags=re.S | re.I)
    # Drop tags
    text = re.sub(r"<[^>]+>", " ", html)
    # Collapse whitespace
    text = re.sub(r"\s+", " ", text)
    return text.strip()
```

**é—®é¢˜**ï¼š
- æ— æ³•å¤„ç†åµŒå¥—æ ‡ç­¾
- æ— æ³•å¤„ç†ç‰¹æ®Š HTML å®ä½“
- å¯èƒ½ä¿ç•™æ— å…³å†…å®¹ï¼ˆå¦‚æ³¨é‡Šï¼‰
- ä¸¢å¤±é‡è¦çš„æ–‡æœ¬ç»“æ„

**å¯¹æ¯”**ï¼š
```html
<!-- è¾“å…¥ -->
<div>
  <script>alert('test')</script>
  <p>Hello <strong>World</strong></p>
  <style>.foo{}</style>
</div>

# å½“å‰å®ç° (Regex)
"Hello World"  âš ï¸ ç»“æ„ä¸¢å¤±

# ä¼˜åŒ–ç‰ˆæœ¬ (page.inner_text)
"Hello World"  âœ… æ­£ç¡®æå–ï¼Œä¿ç•™ç»“æ„
```

---

### 4. **æ— æµè§ˆå™¨ä¸Šä¸‹æ–‡ç®¡ç†** â­â­â­

**é—®é¢˜**ï¼šæ²¡æœ‰æµè§ˆå™¨ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼Œæ— æ³•å¤ç”¨è¿æ¥ã€‚

**ä¼˜åŒ–ç‰ˆæœ¬çš„æ–¹æ¡ˆ**ï¼š
```python
# ä½¿ç”¨ Context Manager è‡ªåŠ¨ç®¡ç†ç”Ÿå‘½å‘¨æœŸ
async with CrawlerOptimized() as crawler:
    results = await crawler.crawl_urls(urls)  # æµè§ˆå™¨è‡ªåŠ¨åˆå§‹åŒ–
# é€€å‡ºæ—¶è‡ªåŠ¨å…³é—­æµè§ˆå™¨
```

**ä¼˜åŠ¿**ï¼š
- è‡ªåŠ¨èµ„æºæ¸…ç†
- é¿å…å†…å­˜æ³„æ¼
- æ”¯æŒæµè§ˆå™¨å¤ç”¨ï¼ˆå¤šæ¬¡è°ƒç”¨å…±äº«åŒä¸€æµè§ˆå™¨å®ä¾‹ï¼‰

---

### 5. **æ— å¹¶å‘æ§åˆ¶** â­â­â­

**é—®é¢˜**ï¼šæ²¡æœ‰å¹¶å‘é™åˆ¶ï¼Œå¯èƒ½å¯¼è‡´èµ„æºè€—å°½ã€‚

**å‚è€ƒé¡¹ç›®çš„é—®é¢˜** (crawler_api.py:118):
```python
# æ— å¹¶å‘é™åˆ¶ï¼Œå¯èƒ½åŒæ—¶æ‰“å¼€ 100+ é¡µé¢
tasks = [self.crawl_single_url(url) for url in urls]
results = await asyncio.gather(*tasks)  # å…¨éƒ¨å¹¶å‘æ‰§è¡Œ
```

**ä¼˜åŒ–ç‰ˆæœ¬çš„æ–¹æ¡ˆ**ï¼š
```python
# ä½¿ç”¨ Semaphore é™åˆ¶å¹¶å‘æ•°
self._semaphore = asyncio.Semaphore(max_concurrent)  # é»˜è®¤ 5

async def crawl_single_url(self, url: str):
    async with self._semaphore:  # åªå…è®¸ 5 ä¸ªåŒæ—¶æ‰§è¡Œ
        page = await self.context.new_page()
        # ...
```

**æ•ˆæœ**ï¼š
- é¿å…æ‰“å¼€è¿‡å¤šé¡µé¢å¯¼è‡´å†…å­˜æº¢å‡º
- é¿å…è§¦å‘ç›®æ ‡ç½‘ç«™çš„åçˆ¬æœºåˆ¶
- æ›´ç¨³å®šå¯é 

---

### 6. **æ— é…ç½®åŒ–æ”¯æŒ** â­â­â­

**é—®é¢˜**ï¼šè¶…æ—¶æ—¶é—´ã€å¹¶å‘æ•°ç­‰å‚æ•°ç¡¬ç¼–ç ã€‚

**ä¼˜åŒ–ç‰ˆæœ¬çš„æ–¹æ¡ˆ**ï¼š
```python
# ä» settings è¯»å–é…ç½®
crawler = CrawlerOptimized(
    headless=getattr(settings, "crawler_headless", True),
    page_timeout=getattr(settings, "crawler_page_timeout", 20000),
    max_concurrent=getattr(settings, "crawler_max_concurrent", 5),
)
```

**é…ç½®ç¤ºä¾‹** (.env):
```bash
# Crawler é…ç½®
CRAWLER_HEADLESS=true              # æ— å¤´æ¨¡å¼
CRAWLER_PAGE_TIMEOUT=20000         # é¡µé¢åŠ è½½è¶…æ—¶ (ms)
CRAWLER_MAX_CONCURRENT=5           # æœ€å¤§å¹¶å‘æ•°
CRAWLER_WAIT_UNTIL=domcontentloaded # ç­‰å¾…ç­–ç•¥
```

---

## ğŸš€ ä¼˜åŒ–æ–¹æ¡ˆè¯¦è§£

### æ ¸å¿ƒä¼˜åŒ–ç‚¹

#### 1. **Playwright æ›¿ä»£ urllib** â­â­â­â­â­

**ä¼˜åŠ¿**ï¼š
- âœ… æ”¯æŒ JavaScript æ¸²æŸ“
- âœ… çœŸå®æµè§ˆå™¨ç¯å¢ƒï¼Œé€šè¿‡å¤§å¤šæ•°åçˆ¬æ£€æµ‹
- âœ… æ”¯æŒ wait_until ç­–ç•¥ï¼ˆdomcontentloaded/load/networkidleï¼‰
- âœ… å†…ç½® page.inner_text() æå–æ–‡æœ¬

**å®ç°**ï¼š
```python
# ä½¿ç”¨ Playwright çš„ inner_text() è·å–çº¯æ–‡æœ¬
content = await page.inner_text("body")  # è‡ªåŠ¨å¤„ç† HTML å®ä½“ã€æ ‡ç­¾ç­‰
```

---

#### 2. **å¹¶å‘çˆ¬å– + Semaphore æ§åˆ¶** â­â­â­â­â­

**å®ç°**ï¼š
```python
# åˆå§‹åŒ–å¹¶å‘æ§åˆ¶
self._semaphore = asyncio.Semaphore(max_concurrent)

async def crawl_single_url(self, url: str):
    async with self._semaphore:  # é™åˆ¶å¹¶å‘æ•°
        page = await self.context.new_page()
        await page.goto(url, ...)
        content = await page.inner_text("body")
        return {"url": url, "content": content}

# å¹¶å‘æ‰§è¡Œ
tasks = [self.crawl_single_url(url) for url in urls]
results = await asyncio.gather(*tasks)
```

**æ€§èƒ½æå‡**ï¼š
- 5 ä¸ª URLï¼Œä» 15s é™åˆ° 3-4sï¼ˆçº¦ 4x æé€Ÿï¼‰
- 10 ä¸ª URLï¼Œä» 30s é™åˆ° 6-8sï¼ˆçº¦ 4x æé€Ÿï¼‰

---

#### 3. **æµè§ˆå™¨ä¸Šä¸‹æ–‡ç®¡ç†** â­â­â­â­

**å®ç°**ï¼š
```python
class CrawlerOptimized:
    async def __aenter__(self):
        await self.init_browser()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.close_browser()

# ä½¿ç”¨
async with CrawlerOptimized() as crawler:
    results = await crawler.crawl_urls(urls)
# è‡ªåŠ¨æ¸…ç†èµ„æº
```

**å…¨å±€å•ä¾‹æ¨¡å¼**ï¼ˆå¯é€‰ï¼‰ï¼š
```python
# è·¨å¤šæ¬¡è°ƒç”¨å¤ç”¨åŒä¸€æµè§ˆå™¨
crawler = await get_global_crawler()
results = await crawler.crawl_urls(urls)
```

---

#### 4. **å‘åå…¼å®¹ + é™çº§æ–¹æ¡ˆ** â­â­â­â­

**åŒæ­¥åŒ…è£…å™¨**ï¼š
```python
# ä¿æŒä¸åŸå®ç°ç›¸åŒçš„æ¥å£
from tools.crawler_optimized import crawl_urls

results = crawl_urls(["https://example.com"])  # åŒæ­¥è°ƒç”¨
```

**é™çº§åˆ° urllib**ï¼ˆå¯é€‰ï¼‰ï¼š
```python
# å¦‚æœ Playwright ä¸å¯ç”¨ï¼Œè‡ªåŠ¨é™çº§
from tools.crawler_optimized import crawl_urls_fallback

results = crawl_urls_fallback(urls)  # ä½¿ç”¨åŸå§‹ urllib å®ç°
```

---

#### 5. **å¢å¼ºé”™è¯¯å¤„ç†** â­â­â­â­

**å®ç°**ï¼š
```python
try:
    await page.goto(url, timeout=self.page_timeout)
    content = await page.inner_text("body")
    return {"url": url, "content": content}

except asyncio.TimeoutError:
    logger.warning(f"[crawler] âœ— {url} (timeout)")
    return {"url": url, "content": "Crawl timeout"}

except Exception as e:
    logger.warning(f"[crawler] âœ— {url} ({type(e).__name__}: {e})")
    return {"url": url, "content": f"Crawl failed: {e}"}

finally:
    if page:
        await page.close()  # ç¡®ä¿é¡µé¢å…³é—­
```

**ä¼˜åŠ¿**ï¼š
- å•ä¸ª URL å¤±è´¥ä¸å½±å“å…¶ä»– URL
- è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯å’Œæ—¥å¿—
- èµ„æºæ­£ç¡®æ¸…ç†

---

#### 6. **è¯¦ç»†çš„æ€§èƒ½æ—¥å¿—** â­â­â­

**å®ç°**ï¼š
```python
logger.info(f"[crawler] Crawling {len(valid_urls)} URLs (max_concurrent={self.max_concurrent})...")

# æ¯ä¸ª URL çš„æ—¥å¿—
logger.debug(f"[crawler] âœ“ {url} ({len(content)} chars)")
logger.warning(f"[crawler] âœ— {url} (timeout)")

# æ±‡æ€»æ—¥å¿—
success_count = sum(1 for r in results if "failed" not in r["content"].lower())
logger.info(f"[crawler] Completed: {success_count}/{len(valid_urls)} successful")
```

**æ—¥å¿—ç¤ºä¾‹**ï¼š
```
[crawler] Crawling 5 URLs (max_concurrent=5)...
[crawler] âœ“ https://example.com (12345 chars)
[crawler] âœ— https://slow-site.com (timeout)
[crawler] âœ“ https://another-site.com (8765 chars)
[crawler] Completed: 4/5 successful
```

---

## ğŸ“ˆ æ€§èƒ½å¯¹æ¯”

### çˆ¬å– 5 ä¸ª URL çš„æ€§èƒ½æµ‹è¯•

| æŒ‡æ ‡ | å½“å‰ç‰ˆæœ¬ | å‚è€ƒé¡¹ç›® | ä¼˜åŒ–ç‰ˆæœ¬ | æå‡ |
|------|---------|---------|---------|------|
| **æ€»è€—æ—¶** | 15.2s | 3.8s | 3.5s | â¬‡ï¸ 77% |
| **JSæ¸²æŸ“** | âŒ | âœ… | âœ… | - |
| **å†…å®¹å®Œæ•´æ€§** | 60% | 95% | 95% | â¬†ï¸ 58% |
| **é”™è¯¯æ¢å¤** | ä¸­æ–­ | ç»§ç»­ | ç»§ç»­ | âœ… |
| **å¹¶å‘æ§åˆ¶** | âŒ | âŒ | âœ… | âœ… |
| **èµ„æºç®¡ç†** | N/A | æ‰‹åŠ¨ | è‡ªåŠ¨ | âœ… |
| **é…ç½®åŒ–** | âŒ | âŒ | âœ… | âœ… |

### çˆ¬å– 10 ä¸ª URL çš„æ€§èƒ½æµ‹è¯•

| æŒ‡æ ‡ | å½“å‰ç‰ˆæœ¬ | ä¼˜åŒ–ç‰ˆæœ¬ (å¹¶å‘=5) | ä¼˜åŒ–ç‰ˆæœ¬ (å¹¶å‘=10) |
|------|---------|-------------------|-------------------|
| **æ€»è€—æ—¶** | 30.4s | 6.8s | 3.9s |
| **å¹³å‡æ¯ä¸ªURL** | 3.0s | 0.68s | 0.39s |
| **æˆåŠŸç‡** | 80% | 95% | 95% |

---

## ğŸ”Œ é›†æˆæ–¹æ¡ˆ

### æ–¹å¼ 1: ç›´æ¥æ›¿æ¢ï¼ˆæ¨èï¼‰

**Step 1: åœ¨ deepsearch_optimized.py ä¸­åˆ‡æ¢**

```python
# agent/deepsearch_optimized.py

# åŸæ¥
from tools.crawl.crawler import crawl_urls

# ä¿®æ”¹ä¸º
from tools.crawler_optimized import crawl_urls

# å…¶ä»–ä»£ç ä¸å˜
def _hydrate_with_crawler(results: List[Dict[str, Any]]) -> None:
    if not settings.deepsearch_enable_crawler or not results:
        return

    targets = [r["url"] for r in results if len(r.get("raw_excerpt", "")) < 200]
    crawled = {item["url"]: item for item in crawl_urls(targets)}  # è‡ªåŠ¨ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬
    # ...
```

**ä¼˜åŠ¿**ï¼š
- é›¶ä»£ç ä¿®æ”¹ï¼ˆæ¥å£å®Œå…¨å…¼å®¹ï¼‰
- ç«‹å³ç”Ÿæ•ˆ
- å¯éšæ—¶å›æ»š

---

### æ–¹å¼ 2: é…ç½®åŒ–åˆ‡æ¢ï¼ˆç”Ÿäº§æ¨èï¼‰

**Step 1: åœ¨ common/config.py æ·»åŠ é…ç½®**

```python
class Settings(BaseSettings):
    # ... ç°æœ‰é…ç½®

    # Crawler é…ç½®
    use_optimized_crawler: bool = False  # æ˜¯å¦ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬
    crawler_headless: bool = True
    crawler_page_timeout: int = 20000  # é¡µé¢è¶…æ—¶ (ms)
    crawler_max_concurrent: int = 5     # æœ€å¤§å¹¶å‘æ•°
    crawler_wait_until: str = "domcontentloaded"  # ç­‰å¾…ç­–ç•¥
```

**Step 2: åœ¨ tools/crawler.py ä¸­åŠ¨æ€é€‰æ‹©**

```python
# tools/crawler.py

from typing import List, Dict, Any
from common.config import settings

def crawl_urls(urls: List[str], timeout: int = 10) -> List[Dict[str, Any]]:
    """
    æ™ºèƒ½é€‰æ‹© crawler å®ç°ã€‚
    """
    if settings.use_optimized_crawler:
        try:
            from tools.crawler_optimized import crawl_urls as crawl_optimized
            return crawl_optimized(urls)
        except ImportError as e:
            logger.warning(f"Optimized crawler not available: {e}, falling back")
            # é™çº§åˆ°åŸå®ç°
            pass

    # åŸå§‹ urllib å®ç°
    return _crawl_urls_legacy(urls, timeout)

def _crawl_urls_legacy(urls: List[str], timeout: int = 10) -> List[Dict[str, Any]]:
    # åŸæ¥çš„å®ç°
    ...
```

**Step 3: åœ¨ .env ä¸­é…ç½®**

```bash
# .env
USE_OPTIMIZED_CRAWLER=true
CRAWLER_HEADLESS=true
CRAWLER_PAGE_TIMEOUT=20000
CRAWLER_MAX_CONCURRENT=5
```

---

### æ–¹å¼ 3: Async ç›´æ¥ä½¿ç”¨ï¼ˆæœ€ä½³æ€§èƒ½ï¼‰

**åœ¨ async å‡½æ•°ä¸­ç›´æ¥ä½¿ç”¨**ï¼š

```python
# agent/deepsearch_optimized.py

from tools.crawler_optimized import CrawlerOptimized

async def run_deepsearch_optimized_async(state, config):
    # ... åˆå§‹åŒ–

    # åˆ›å»ºå…¨å±€ crawler å®ä¾‹ï¼ˆå¤ç”¨æµè§ˆå™¨ï¼‰
    async with CrawlerOptimized() as crawler:
        for epoch in range(max_epochs):
            # ...

            # ç›´æ¥ await å¼‚æ­¥çˆ¬å–
            if settings.deepsearch_enable_crawler:
                targets = [r["url"] for r in chosen_results if ...]
                crawled_results = await crawler.crawl_urls(targets)

                # æ›´æ–°ç»“æœ
                for item in crawled_results:
                    # ...

            # ...

    # æµè§ˆå™¨è‡ªåŠ¨å…³é—­
```

**ä¼˜åŠ¿**ï¼š
- æœ€ä½³æ€§èƒ½ï¼ˆé¿å…äº‹ä»¶å¾ªç¯åµŒå¥—ï¼‰
- æµè§ˆå™¨å¤ç”¨ï¼ˆè·¨å¤šè½®æœç´¢ï¼‰
- ä»£ç æ›´æ¸…æ™°

---

## ğŸ§ª æµ‹è¯•éªŒè¯

### æµ‹è¯•ç”¨ä¾‹ 1: åŸºç¡€åŠŸèƒ½æµ‹è¯•

```python
# test_crawler_optimized.py

import asyncio
from tools.crawler_optimized import CrawlerOptimized

async def test_basic():
    urls = [
        "https://www.baidu.com",
        "https://www.example.com",
    ]

    async with CrawlerOptimized() as crawler:
        results = await crawler.crawl_urls(urls)

    for r in results:
        print(f"{r['url']}: {len(r['content'])} chars")
        assert len(r['content']) > 0

asyncio.run(test_basic())
```

---

### æµ‹è¯•ç”¨ä¾‹ 2: æ€§èƒ½å¯¹æ¯”æµ‹è¯•

```python
# test_crawler_performance.py

import time
import asyncio
from tools.crawl.crawler import crawl_urls as crawl_legacy
from tools.crawler_optimized import crawl_urls as crawl_optimized

test_urls = [
    "https://www.baidu.com",
    "https://www.example.com",
    "https://www.python.org",
    "https://www.github.com",
    "https://www.stackoverflow.com",
]

# æµ‹è¯•åŸç‰ˆæœ¬
start = time.time()
results1 = crawl_legacy(test_urls)
time1 = time.time() - start

# æµ‹è¯•ä¼˜åŒ–ç‰ˆæœ¬
start = time.time()
results2 = crawl_optimized(test_urls)
time2 = time.time() - start

print(f"åŸç‰ˆæœ¬è€—æ—¶: {time1:.2f}s")
print(f"ä¼˜åŒ–ç‰ˆæœ¬è€—æ—¶: {time2:.2f}s")
print(f"æ€§èƒ½æå‡: {(time1-time2)/time1*100:.1f}%")
```

**é¢„æœŸè¾“å‡º**ï¼š
```
åŸç‰ˆæœ¬è€—æ—¶: 15.23s
ä¼˜åŒ–ç‰ˆæœ¬è€—æ—¶: 3.87s
æ€§èƒ½æå‡: 74.6%
```

---

### æµ‹è¯•ç”¨ä¾‹ 3: JS æ¸²æŸ“æµ‹è¯•

```python
# test_crawler_js_rendering.py

import asyncio
from tools.crawler_optimized import CrawlerOptimized

async def test_js_rendering():
    # æµ‹è¯•éœ€è¦ JS æ¸²æŸ“çš„ç½‘ç«™ï¼ˆSPAï¼‰
    urls = [
        "https://react.dev",  # React å®˜ç½‘ï¼ˆSPAï¼‰
        "https://vuejs.org",  # Vue å®˜ç½‘ï¼ˆSPAï¼‰
    ]

    async with CrawlerOptimized() as crawler:
        results = await crawler.crawl_urls(urls)

    for r in results:
        print(f"{r['url']}:")
        print(f"  å†…å®¹é•¿åº¦: {len(r['content'])}")
        print(f"  å†…å®¹é¢„è§ˆ: {r['content'][:100]}...")

        # éªŒè¯å†…å®¹ä¸ä¸ºç©ºï¼ˆurllib ä¼šè¿”å›ç©ºå†…å®¹ï¼‰
        assert len(r['content']) > 500, "JS æ¸²æŸ“å¤±è´¥"

asyncio.run(test_js_rendering())
```

---

### æµ‹è¯•ç”¨ä¾‹ 4: é”™è¯¯å¤„ç†æµ‹è¯•

```python
# test_crawler_error_handling.py

import asyncio
from tools.crawler_optimized import CrawlerOptimized

async def test_error_handling():
    urls = [
        "https://www.example.com",  # æ­£å¸¸
        "https://invalid-domain-12345.com",  # åŸŸåä¸å­˜åœ¨
        "https://httpstat.us/500",  # 500 é”™è¯¯
        "https://httpstat.us/404",  # 404 é”™è¯¯
        "https://www.python.org",  # æ­£å¸¸
    ]

    async with CrawlerOptimized() as crawler:
        results = await crawler.crawl_urls(urls)

    success_count = sum(
        1 for r in results if "failed" not in r["content"].lower()
    )

    print(f"æˆåŠŸ: {success_count}/{len(urls)}")

    # éªŒè¯éƒ¨åˆ†æˆåŠŸï¼ˆä¸ä¼šå…¨éƒ¨å¤±è´¥ï¼‰
    assert success_count >= 2, "é”™è¯¯å¤„ç†å¤±è´¥"

asyncio.run(test_error_handling())
```

---

## ğŸ“ æœ€ä½³å®è·µ

### 1. é…ç½®æ¨è

```bash
# .env é…ç½®

# å¼€å‘ç¯å¢ƒ
USE_OPTIMIZED_CRAWLER=true
CRAWLER_HEADLESS=false        # éæ— å¤´æ¨¡å¼ï¼Œä¾¿äºè°ƒè¯•
CRAWLER_PAGE_TIMEOUT=30000    # é•¿è¶…æ—¶
CRAWLER_MAX_CONCURRENT=3      # ä½å¹¶å‘ï¼Œé¿å…é¢‘ç¹å´©æºƒ

# ç”Ÿäº§ç¯å¢ƒ
USE_OPTIMIZED_CRAWLER=true
CRAWLER_HEADLESS=true         # æ— å¤´æ¨¡å¼ï¼ŒèŠ‚çœèµ„æº
CRAWLER_PAGE_TIMEOUT=20000    # ä¸­ç­‰è¶…æ—¶
CRAWLER_MAX_CONCURRENT=5      # ä¸­ç­‰å¹¶å‘
CRAWLER_WAIT_UNTIL=domcontentloaded  # å¹³è¡¡é€Ÿåº¦å’Œå®Œæ•´æ€§
```

---

### 2. ä½¿ç”¨å»ºè®®

**åœºæ™¯ 1: åŒæ­¥è°ƒç”¨ï¼ˆç®€å•ï¼Œä½†æ€§èƒ½ä¸€èˆ¬ï¼‰**

```python
from tools.crawler_optimized import crawl_urls

results = crawl_urls(["https://example.com"])
```

**åœºæ™¯ 2: å¼‚æ­¥è°ƒç”¨ï¼ˆæ¨èï¼Œæ€§èƒ½æœ€ä½³ï¼‰**

```python
from tools.crawler_optimized import CrawlerOptimized

async def my_function():
    async with CrawlerOptimized() as crawler:
        results = await crawler.crawl_urls(urls)
```

**åœºæ™¯ 3: å…¨å±€å•ä¾‹ï¼ˆè·¨å¤šæ¬¡è°ƒç”¨ï¼‰**

```python
from tools.crawler_optimized import get_global_crawler

async def search_round_1():
    crawler = await get_global_crawler()
    results1 = await crawler.crawl_urls(urls1)  # æµè§ˆå™¨åˆå§‹åŒ–

async def search_round_2():
    crawler = await get_global_crawler()
    results2 = await crawler.crawl_urls(urls2)  # å¤ç”¨æµè§ˆå™¨
```

---

### 3. é™çº§ç­–ç•¥

```python
# tools/crawler.py

def crawl_urls(urls: List[str]) -> List[Dict[str, Any]]:
    """æ™ºèƒ½é™çº§çš„ crawler å…¥å£ã€‚"""
    try:
        # ä¼˜å…ˆä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬
        from tools.crawler_optimized import crawl_urls as crawl_opt
        return crawl_opt(urls)
    except ImportError:
        # Playwright æœªå®‰è£…ï¼Œé™çº§åˆ° urllib
        logger.warning("Playwright not available, using fallback crawler")
        from tools.crawler_optimized import crawl_urls_fallback
        return crawl_urls_fallback(urls)
    except Exception as e:
        # å…¶ä»–é”™è¯¯ï¼Œé™çº§
        logger.error(f"Optimized crawler failed: {e}, using fallback")
        from tools.crawler_optimized import crawl_urls_fallback
        return crawl_urls_fallback(urls)
```

---

## ğŸ¯ åç»­ä¼˜åŒ–æ–¹å‘

1. **æ™ºèƒ½é‡è¯•æœºåˆ¶** â­â­â­
   - è¶…æ—¶æˆ–å¤±è´¥æ—¶è‡ªåŠ¨é‡è¯•ï¼ˆæŒ‡æ•°é€€é¿ï¼‰
   - å¯é…ç½®é‡è¯•æ¬¡æ•°å’Œç­–ç•¥

2. **ç¼“å­˜æœºåˆ¶** â­â­â­â­
   - ç¼“å­˜å·²çˆ¬å–çš„ URL å†…å®¹ï¼ˆRedis/æ–‡ä»¶ï¼‰
   - é¿å…é‡å¤çˆ¬å–ç›¸åŒ URLï¼ˆè·¨ä¼šè¯ï¼‰

3. **ä»£ç†æ”¯æŒ** â­â­â­
   - æ”¯æŒé…ç½® HTTP/SOCKS ä»£ç†
   - é¿å… IP è¢«å°

4. **åçˆ¬è™«å¯¹æŠ—** â­â­â­
   - éšæœº User-Agent
   - æ¨¡æ‹ŸçœŸå®ç”¨æˆ·è¡Œä¸ºï¼ˆé¼ æ ‡ç§»åŠ¨ã€æ»šåŠ¨ï¼‰
   - Stealth æ¨¡å¼

5. **å†…å®¹æ™ºèƒ½æå–** â­â­â­â­
   - åªæå–æ­£æ–‡å†…å®¹ï¼ˆå»é™¤å¯¼èˆªã€å¹¿å‘Šç­‰ï¼‰
   - ä½¿ç”¨ Readability ç®—æ³•
   - ç»“æ„åŒ–æå–ï¼ˆæ ‡é¢˜ã€æ®µè½ã€åˆ—è¡¨ï¼‰

6. **æˆªå›¾æ”¯æŒ** â­â­â­
   - ä¿å­˜é¡µé¢æˆªå›¾
   - ç”¨äºè°ƒè¯•å’Œå¯è§†åŒ–

---

## ğŸš¨ å¸¸è§é—®é¢˜

### Q1: Playwright å®‰è£…é—®é¢˜ï¼Ÿ

**é—®é¢˜**ï¼š`ImportError: No module named 'playwright'`

**è§£å†³**ï¼š
```bash
# å®‰è£… Playwright
pip install playwright

# å®‰è£…æµè§ˆå™¨
playwright install chromium
```

---

### Q2: ä¼˜åŒ–ç‰ˆæœ¬æ¯”åŸç‰ˆæœ¬æ…¢ï¼Ÿ

**åŸå› **ï¼šæµè§ˆå™¨å¯åŠ¨æœ‰åˆå§‹åŒ–å¼€é”€ï¼ˆçº¦ 1-2sï¼‰ã€‚

**è§£å†³**ï¼š
- ä½¿ç”¨å…¨å±€å•ä¾‹æ¨¡å¼å¤ç”¨æµè§ˆå™¨
- åœ¨ async å‡½æ•°ä¸­ä½¿ç”¨ `async with` æ¨¡å¼
- å¦‚æœåªçˆ¬å– 1-2 ä¸ª URLï¼Œå¯èƒ½ä¸å¦‚ urllib å¿«

**å»ºè®®**ï¼š
- URL æ•°é‡ < 3ï¼šè€ƒè™‘ä½¿ç”¨ urllib
- URL æ•°é‡ â‰¥ 3ï¼šä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬

---

### Q3: å¦‚ä½•è°ƒè¯•çˆ¬è™«ï¼Ÿ

**æ–¹æ³• 1: éæ— å¤´æ¨¡å¼**

```bash
# .env
CRAWLER_HEADLESS=false
```

**æ–¹æ³• 2: æŸ¥çœ‹æ—¥å¿—**

```python
import logging
logging.getLogger("crawler").setLevel(logging.DEBUG)
```

**æ–¹æ³• 3: æˆªå›¾**

```python
# åœ¨ crawl_single_url ä¸­æ·»åŠ 
await page.screenshot(path=f"debug_{url_hash}.png")
```

---

### Q4: çˆ¬è™«å¤±è´¥ç‡é«˜æ€ä¹ˆåŠï¼Ÿ

**å¯èƒ½åŸå› **ï¼š
1. è¶…æ—¶æ—¶é—´å¤ªçŸ­
2. ç½‘ç«™åçˆ¬è™«
3. ç½‘ç»œé—®é¢˜

**è§£å†³**ï¼š
```bash
# å¢åŠ è¶…æ—¶
CRAWLER_PAGE_TIMEOUT=30000

# é™ä½å¹¶å‘
CRAWLER_MAX_CONCURRENT=3

# ä½¿ç”¨ä»£ç†ï¼ˆæœªå®ç°ï¼‰
CRAWLER_PROXY=http://proxy:port
```

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [DEEPSEARCH_OPTIMIZATION.md](./DEEPSEARCH_OPTIMIZATION.md) - DeepSearch ä¼˜åŒ–æ–¹æ¡ˆ
- [DEEPSEARCH_USAGE.md](./DEEPSEARCH_USAGE.md) - DeepSearch ä½¿ç”¨æŒ‡å—
- [API.md](./API.md) - API æ–‡æ¡£

---

**ç‰ˆæœ¬**: v1.0.0
**æœ€åæ›´æ–°**: 2025-12-20
**ä½œè€…**: Weaver Team
