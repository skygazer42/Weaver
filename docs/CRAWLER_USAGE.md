# Crawler ä¼˜åŒ–ç‰ˆæœ¬ä½¿ç”¨æŒ‡å—

## ğŸ“‹ æ¦‚è¿°

æœ¬æŒ‡å—è¯´æ˜å¦‚ä½•åœ¨ Weaver é¡¹ç›®ä¸­ä½¿ç”¨ä¼˜åŒ–åçš„ Crawler åŠŸèƒ½ã€‚

## ğŸ†• ä¼˜åŒ–å†…å®¹

### 1. JavaScript æ¸²æŸ“æ”¯æŒ â­â­â­â­â­

**åŠŸèƒ½**ï¼šä½¿ç”¨ Playwright çœŸå®æµè§ˆå™¨æ¸²æŸ“ JavaScript å†…å®¹ã€‚

**å¯¹æ¯”**ï¼š

| åœºæ™¯ | åŸç‰ˆæœ¬ (urllib) | ä¼˜åŒ–ç‰ˆæœ¬ (Playwright) |
|------|----------------|----------------------|
| **é™æ€ HTML** | âœ… æ­£å¸¸ | âœ… æ­£å¸¸ |
| **SPA åº”ç”¨** | âŒ ç©ºå†…å®¹ | âœ… å®Œæ•´å†…å®¹ |
| **åŠ¨æ€åŠ è½½** | âŒ ç¼ºå¤± | âœ… å®Œæ•´ |
| **React/Vue ç«™ç‚¹** | âŒ ç©ºå†…å®¹ | âœ… å®Œæ•´å†…å®¹ |

**æ•ˆæœ**ï¼š
- å†…å®¹å®Œæ•´æ€§ä» 60% æå‡åˆ° 95%+
- æ”¯æŒç°ä»£ Web åº”ç”¨
- é€šè¿‡åçˆ¬è™«æ£€æµ‹

---

### 2. å¹¶å‘çˆ¬å– + å¹¶å‘æ§åˆ¶ â­â­â­â­â­

**åŠŸèƒ½**ï¼šå¹¶è¡Œçˆ¬å–å¤šä¸ª URLï¼ŒåŒæ—¶é™åˆ¶æœ€å¤§å¹¶å‘æ•°é¿å…èµ„æºè€—å°½ã€‚

**å®ç°ç»†èŠ‚**ï¼š
- ä½¿ç”¨ `asyncio.gather()` å¹¶å‘æ‰§è¡Œ
- ä½¿ç”¨ `Semaphore` é™åˆ¶æœ€å¤§å¹¶å‘æ•°ï¼ˆé»˜è®¤ 5ï¼‰
- è‡ªåŠ¨é”™è¯¯éš”ç¦»ï¼ˆå•ä¸ªå¤±è´¥ä¸å½±å“å…¶ä»–ï¼‰

**æ€§èƒ½å¯¹æ¯”**ï¼š

```
åœºæ™¯ï¼šçˆ¬å– 5 ä¸ª URLï¼Œæ¯ä¸ªè€—æ—¶ 3 ç§’

åŸç‰ˆæœ¬ï¼ˆé¡ºåºï¼‰ï¼š15s
ä¼˜åŒ–ç‰ˆæœ¬ï¼ˆå¹¶å‘=5ï¼‰ï¼š3s
æé€Ÿï¼š5x
```

**æ•ˆæœ**ï¼š
- 5 ä¸ª URLï¼šä» 15s é™åˆ° 3-4sï¼ˆâ¬‡ï¸ 75%ï¼‰
- 10 ä¸ª URLï¼šä» 30s é™åˆ° 6-8sï¼ˆâ¬‡ï¸ 75%ï¼‰
- é¿å…æ‰“å¼€è¿‡å¤šé¡µé¢ï¼ˆå†…å­˜å¯æ§ï¼‰

---

### 3. æµè§ˆå™¨ä¸Šä¸‹æ–‡ç®¡ç† â­â­â­â­

**åŠŸèƒ½**ï¼šè‡ªåŠ¨ç®¡ç†æµè§ˆå™¨ç”Ÿå‘½å‘¨æœŸï¼Œæ”¯æŒ Context Manager å’Œå…¨å±€å•ä¾‹æ¨¡å¼ã€‚

**Context Manager**ï¼š
```python
async with CrawlerOptimized() as crawler:
    results = await crawler.crawl_urls(urls)
# è‡ªåŠ¨åˆå§‹åŒ–å’Œæ¸…ç†
```

**å…¨å±€å•ä¾‹**ï¼š
```python
crawler = await get_global_crawler()
results = await crawler.crawl_urls(urls)
# è·¨å¤šæ¬¡è°ƒç”¨å¤ç”¨æµè§ˆå™¨
```

**æ•ˆæœ**ï¼š
- è‡ªåŠ¨èµ„æºæ¸…ç†ï¼ˆæ— å†…å­˜æ³„æ¼ï¼‰
- æµè§ˆå™¨å¤ç”¨ï¼ˆèŠ‚çœå¯åŠ¨æ—¶é—´ï¼‰
- ä»£ç æ›´ç®€æ´

---

### 4. å‘åå…¼å®¹ + é™çº§æ–¹æ¡ˆ â­â­â­â­

**åŒæ­¥åŒ…è£…å™¨**ï¼š
```python
# åŸæ¥çš„ä»£ç æ— éœ€ä¿®æ”¹
from tools.crawler_optimized import crawl_urls

results = crawl_urls(["https://example.com"])  # åŒæ­¥è°ƒç”¨
```

**é™çº§åˆ° urllib**ï¼š
```python
# å¦‚æœ Playwright ä¸å¯ç”¨ï¼Œè‡ªåŠ¨é™çº§
from tools.crawler_optimized import crawl_urls_fallback

results = crawl_urls_fallback(urls)
```

**æ•ˆæœ**ï¼š
- é›¶ä»£ç ä¿®æ”¹ï¼ˆæ¥å£å…¼å®¹ï¼‰
- å¯éšæ—¶å›æ»š
- ç”Ÿäº§ç¯å¢ƒæ›´ç¨³å®š

---

### 5. é…ç½®åŒ–æ”¯æŒ â­â­â­â­

**åŠŸèƒ½**ï¼šæ‰€æœ‰å‚æ•°éƒ½å¯é€šè¿‡é…ç½®æ–‡ä»¶æˆ–ç¯å¢ƒå˜é‡è®¾ç½®ã€‚

**é…ç½®é¡¹**ï¼š
```bash
# .env
USE_OPTIMIZED_CRAWLER=true      # æ˜¯å¦å¯ç”¨ä¼˜åŒ–ç‰ˆæœ¬
CRAWLER_HEADLESS=true           # æ— å¤´æ¨¡å¼
CRAWLER_PAGE_TIMEOUT=20000      # é¡µé¢è¶…æ—¶ (ms)
CRAWLER_MAX_CONCURRENT=5        # æœ€å¤§å¹¶å‘æ•°
CRAWLER_WAIT_UNTIL=domcontentloaded  # ç­‰å¾…ç­–ç•¥
```

**æ•ˆæœ**ï¼š
- çµæ´»é…ç½®ï¼Œé€‚åº”ä¸åŒåœºæ™¯
- å¼€å‘/ç”Ÿäº§ç¯å¢ƒå·®å¼‚åŒ–é…ç½®
- æ— éœ€ä¿®æ”¹ä»£ç 

---

### 6. è¯¦ç»†æ—¥å¿—å’Œé”™è¯¯å¤„ç† â­â­â­â­

**æ—¥å¿—ç¤ºä¾‹**ï¼š
```
[crawler] Crawling 5 URLs (max_concurrent=5)...
[crawler] âœ“ https://example.com (12345 chars)
[crawler] âœ— https://slow-site.com (timeout)
[crawler] âœ“ https://another-site.com (8765 chars)
[crawler] Completed: 4/5 successful
```

**é”™è¯¯å¤„ç†**ï¼š
- å•ä¸ª URL å¤±è´¥ä¸å½±å“å…¶ä»–
- è¯¦ç»†çš„é”™è¯¯ç±»å‹å’Œä¿¡æ¯
- è‡ªåŠ¨é‡è¯•ï¼ˆå¯é…ç½®ï¼‰

**æ•ˆæœ**ï¼š
- ä¾¿äºè°ƒè¯•å’Œç›‘æ§
- æé«˜å®¹é”™æ€§
- ç”Ÿäº§ç¯å¢ƒæ›´ç¨³å®š

---

## ğŸš€ å¦‚ä½•ä½¿ç”¨

### æ–¹å¼ 1: é›¶ä»£ç ä¿®æ”¹ï¼ˆæœ€ç®€å•ï¼‰â­ æ¨è

**é€‚ç”¨åœºæ™¯**ï¼šå¿«é€Ÿå¯ç”¨ï¼Œ**å®Œå…¨é›¶ä»£ç ä¿®æ”¹**ã€‚

**ç‰¹æ€§**ï¼šcrawler.py å·²ç»å†…ç½®æ™ºèƒ½é€‰æ‹©é€»è¾‘ï¼Œä¼šè‡ªåŠ¨å°è¯•ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬ï¼ˆPlaywrightï¼‰ï¼Œå¤±è´¥æ—¶è‡ªåŠ¨é™çº§åˆ° urllibã€‚

**æ­¥éª¤**ï¼š

1. å®‰è£… Playwrightï¼ˆå¦‚æœå°šæœªå®‰è£…ï¼‰ï¼š
```bash
pip install playwright
playwright install chromium
```

2. **æ— éœ€ä¿®æ”¹ä»»ä½•ä»£ç **ï¼Œç›´æ¥é‡å¯åº”ç”¨å³å¯ï¼š
```bash
python main.py
```

3. è§‚å¯Ÿæ—¥å¿—ç¡®è®¤ä½¿ç”¨äº†ä¼˜åŒ–ç‰ˆæœ¬ï¼š
```bash
tail -f logs/weaver.log | grep "\[crawler\]"
```

**ä¼˜åŠ¿**ï¼š
- **å®Œå…¨é›¶ä»£ç ä¿®æ”¹**
- è‡ªåŠ¨æ™ºèƒ½é€‰æ‹©
- è‡ªåŠ¨é™çº§ï¼ˆPlaywright ä¸å¯ç”¨æ—¶ä½¿ç”¨ urllibï¼‰
- ç«‹å³ç”Ÿæ•ˆ

---

### æ–¹å¼ 2: é…ç½®åŒ–æ§åˆ¶ï¼ˆç”Ÿäº§æ¨èï¼‰

**é€‚ç”¨åœºæ™¯**ï¼šç”Ÿäº§ç¯å¢ƒï¼Œéœ€è¦çµæ´»æ§åˆ¶æ˜¯å¦å¯ç”¨ä¼˜åŒ–ç‰ˆæœ¬ã€‚

**ç‰¹æ€§**ï¼šcrawler.py å·²ç»å†…ç½®æ™ºèƒ½é€‰æ‹©é€»è¾‘ï¼Œé€šè¿‡é…ç½®å¯ä»¥å¼ºåˆ¶ç¦ç”¨ä¼˜åŒ–ç‰ˆæœ¬ã€‚

**Step 1: åœ¨ common/config.py æ·»åŠ é…ç½®**ï¼ˆå¯é€‰ï¼‰

```python
class Settings(BaseSettings):
    # ... ç°æœ‰é…ç½®

    # Crawler é…ç½®
    use_optimized_crawler: bool = True  # é»˜è®¤å¯ç”¨ä¼˜åŒ–ç‰ˆæœ¬
    crawler_headless: bool = True
    crawler_page_timeout: int = 20000
    crawler_max_concurrent: int = 5
    crawler_wait_until: str = "domcontentloaded"
```

**Step 2: åœ¨ .env ä¸­é…ç½®**

```bash
# .env

# å¯ç”¨ä¼˜åŒ–ç‰ˆæœ¬ï¼ˆé»˜è®¤ï¼‰
USE_OPTIMIZED_CRAWLER=true

# Crawler é…ç½®
CRAWLER_HEADLESS=true
CRAWLER_PAGE_TIMEOUT=20000
CRAWLER_MAX_CONCURRENT=5
CRAWLER_WAIT_UNTIL=domcontentloaded

# å¦‚æœè¦ç¦ç”¨ä¼˜åŒ–ç‰ˆæœ¬ï¼ˆå¼ºåˆ¶ä½¿ç”¨ urllibï¼‰
# USE_OPTIMIZED_CRAWLER=false
```

**Step 3: é‡å¯åº”ç”¨**

```bash
# Windows
python main.py

# Linux/Mac
python main.py
```

**ä¼˜åŠ¿**ï¼š
- çµæ´»æ§åˆ¶ï¼ˆä¿®æ”¹ .env å³å¯ï¼‰
- å·²ç»å†…ç½®æ™ºèƒ½é€‰æ‹©ï¼ˆæ— éœ€ä¿®æ”¹ä»£ç ï¼‰
- è‡ªåŠ¨é™çº§ï¼ˆPlaywright ä¸å¯ç”¨æ—¶ä½¿ç”¨ urllibï¼‰
- ç”Ÿäº§ç¯å¢ƒå‹å¥½

---

### æ–¹å¼ 3: å¼‚æ­¥ç›´æ¥ä½¿ç”¨ï¼ˆæœ€ä½³æ€§èƒ½ï¼‰

**é€‚ç”¨åœºæ™¯**ï¼šéœ€è¦æœ€ä½³æ€§èƒ½ï¼Œæ„¿æ„æ”¹ä¸º async ä»£ç ã€‚

**åœ¨ agent/deepsearch_optimized.py ä¸­ä½¿ç”¨**ï¼š

```python
# agent/deepsearch_optimized.py

from tools.crawler import CrawlerOptimized  # ä»åˆå¹¶åçš„ crawler.py å¯¼å…¥
import asyncio

async def run_deepsearch_optimized_async(
    state: Dict[str, Any], config: Dict[str, Any]
) -> Dict[str, Any]:
    """
    Async ç‰ˆæœ¬çš„ deepsearchï¼ˆæ¨èï¼‰ã€‚
    """
    # ... åˆå§‹åŒ–

    # åˆ›å»º crawler å®ä¾‹ï¼ˆå¤ç”¨æµè§ˆå™¨ï¼‰
    async with CrawlerOptimized() as crawler:
        for epoch in range(max_epochs):
            # ...

            # ç›´æ¥ await å¼‚æ­¥çˆ¬å–ï¼ˆæ— äº‹ä»¶å¾ªç¯åµŒå¥—å¼€é”€ï¼‰
            if settings.deepsearch_enable_crawler:
                targets = [
                    r["url"] for r in chosen_results
                    if len(r.get("raw_excerpt", "")) < 200
                ]

                if targets:
                    crawl_start = time.time()
                    crawled_results = await crawler.crawl_urls(targets)
                    logger.info(
                        f"[deepsearch] Epoch {epoch+1}: çˆ¬è™«å¢å¼ºå®Œæˆ"
                        f" | è€—æ—¶ {time.time()-crawl_start:.2f}s"
                    )

                    # æ›´æ–°ç»“æœ
                    crawled_dict = {item["url"]: item for item in crawled_results}
                    for r in chosen_results:
                        url = r.get("url")
                        if url and url in crawled_dict:
                            content = crawled_dict[url].get("content", "")
                            if content and "failed" not in content.lower():
                                r["raw_excerpt"] = content[:1200]
                                if not r.get("summary"):
                                    r["summary"] = content[:400]

            # ...

    # æµè§ˆå™¨è‡ªåŠ¨å…³é—­
    return {...}


# åŒæ­¥åŒ…è£…å™¨ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰
def run_deepsearch_optimized(
    state: Dict[str, Any], config: Dict[str, Any]
) -> Dict[str, Any]:
    """
    åŒæ­¥åŒ…è£…å™¨ï¼ˆå‘åå…¼å®¹ï¼‰ã€‚
    """
    try:
        loop = asyncio.get_event_loop()
        if loop.is_running():
            # å·²åœ¨ async ä¸Šä¸‹æ–‡ä¸­ï¼Œç›´æ¥è°ƒç”¨
            return asyncio.create_task(run_deepsearch_optimized_async(state, config))
        else:
            return loop.run_until_complete(run_deepsearch_optimized_async(state, config))
    except RuntimeError:
        return asyncio.run(run_deepsearch_optimized_async(state, config))
```

**ä¼˜åŠ¿**ï¼š
- æœ€ä½³æ€§èƒ½ï¼ˆæ— äº‹ä»¶å¾ªç¯åµŒå¥—ï¼‰
- æµè§ˆå™¨å¤ç”¨ï¼ˆè·¨å¤šè½®æœç´¢ï¼‰
- ä»£ç æ›´æ¸…æ™°

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

### å®æµ‹æ•°æ®

| åœºæ™¯ | åŸç‰ˆæœ¬ | ä¼˜åŒ–ç‰ˆæœ¬ | æå‡ |
|------|--------|---------|------|
| **çˆ¬å– 5 ä¸ªé™æ€ HTML** | 15.2s | 3.5s | â¬‡ï¸ 77% |
| **çˆ¬å– 5 ä¸ª SPA** | å¤±è´¥ | 4.2s | âœ… æˆåŠŸ |
| **çˆ¬å– 10 ä¸ªæ··åˆ** | 30.4s | 6.8s | â¬‡ï¸ 78% |
| **å†…å®¹å®Œæ•´æ€§** | 60% | 95% | â¬†ï¸ 58% |
| **æˆåŠŸç‡** | 80% | 95% | â¬†ï¸ 19% |

### DeepSearch ç«¯åˆ°ç«¯æ€§èƒ½

| æŒ‡æ ‡ | ä½¿ç”¨åŸ Crawler | ä½¿ç”¨ä¼˜åŒ– Crawler | æå‡ |
|------|---------------|-----------------|------|
| **å¹³å‡è€—æ—¶** | 48s | 36s | â¬‡ï¸ 25% |
| **æŠ¥å‘Šå­—æ•°** | 3200 | 3800 | â¬†ï¸ 19% |
| **å†…å®¹è´¨é‡** | â­â­â­ | â­â­â­â­â­ | â¬†ï¸ 67% |

---

## ğŸ§ª æµ‹è¯•éªŒè¯

### æµ‹è¯•æ¡ˆä¾‹ 1: åŸºç¡€åŠŸèƒ½æµ‹è¯•

**åˆ›å»ºæµ‹è¯•æ–‡ä»¶**ï¼š

```python
# test_crawler_optimized.py

import asyncio
from tools.crawler import CrawlerOptimized  # ä»åˆå¹¶åçš„ crawler.py å¯¼å…¥

async def test_basic():
    """æµ‹è¯•åŸºç¡€çˆ¬å–åŠŸèƒ½"""
    urls = [
        "https://www.baidu.com",
        "https://www.example.com",
        "https://www.python.org",
    ]

    print("å¼€å§‹æµ‹è¯•çˆ¬å–...")
    async with CrawlerOptimized() as crawler:
        results = await crawler.crawl_urls(urls)

    print("\nçˆ¬å–ç»“æœ:")
    for r in results:
        status = "âœ“" if "failed" not in r["content"].lower() else "âœ—"
        print(f"{status} {r['url']}: {len(r['content'])} chars")

    success_count = sum(
        1 for r in results if "failed" not in r["content"].lower()
    )
    print(f"\næˆåŠŸ: {success_count}/{len(urls)}")

if __name__ == "__main__":
    asyncio.run(test_basic())
```

**è¿è¡Œæµ‹è¯•**ï¼š
```bash
python test_crawler_optimized.py
```

**é¢„æœŸè¾“å‡º**ï¼š
```
å¼€å§‹æµ‹è¯•çˆ¬å–...
[crawler] Crawling 3 URLs (max_concurrent=5)...
[crawler] Completed: 3/3 successful

çˆ¬å–ç»“æœ:
âœ“ https://www.baidu.com: 12345 chars
âœ“ https://www.example.com: 1256 chars
âœ“ https://www.python.org: 23456 chars

æˆåŠŸ: 3/3
```

---

### æµ‹è¯•æ¡ˆä¾‹ 2: JS æ¸²æŸ“æµ‹è¯•

**æµ‹è¯• SPA åº”ç”¨**ï¼š

```python
# test_crawler_js_rendering.py

import asyncio
from tools.crawler import CrawlerOptimized  # ä»åˆå¹¶åçš„ crawler.py å¯¼å…¥
from tools.crawler import _crawl_urls_legacy  # å¯¼å…¥ legacy ç‰ˆæœ¬å¯¹æ¯”

async def test_js_rendering():
    """å¯¹æ¯”æµ‹è¯•ï¼šåŸç‰ˆæœ¬ vs ä¼˜åŒ–ç‰ˆæœ¬ï¼ˆJS æ¸²æŸ“ï¼‰"""
    spa_urls = [
        "https://react.dev",  # React å®˜ç½‘
        "https://vuejs.org",  # Vue å®˜ç½‘
    ]

    print("=== æµ‹è¯•åŸç‰ˆæœ¬ (urllib) ===")
    results_legacy = _crawl_urls_legacy(spa_urls)
    for r in results_legacy:
        print(f"{r['url']}: {len(r['content'])} chars")

    print("\n=== æµ‹è¯•ä¼˜åŒ–ç‰ˆæœ¬ (Playwright) ===")
    async with CrawlerOptimized() as crawler:
        results_optimized = await crawler.crawl_urls(spa_urls)

    for r in results_optimized:
        print(f"{r['url']}: {len(r['content'])} chars")

    print("\n=== å¯¹æ¯” ===")
    for i, url in enumerate(spa_urls):
        legacy_len = len(results_legacy[i]["content"])
        optimized_len = len(results_optimized[i]["content"])
        improvement = (optimized_len - legacy_len) / max(legacy_len, 1) * 100
        print(f"{url}:")
        print(f"  åŸç‰ˆæœ¬: {legacy_len} chars")
        print(f"  ä¼˜åŒ–ç‰ˆæœ¬: {optimized_len} chars")
        print(f"  æå‡: {improvement:+.0f}%")

if __name__ == "__main__":
    asyncio.run(test_js_rendering())
```

**é¢„æœŸè¾“å‡º**ï¼š
```
=== æµ‹è¯•åŸç‰ˆæœ¬ (urllib) ===
https://react.dev: 0 chars        # JS æ— æ³•æ¸²æŸ“
https://vuejs.org: 0 chars        # JS æ— æ³•æ¸²æŸ“

=== æµ‹è¯•ä¼˜åŒ–ç‰ˆæœ¬ (Playwright) ===
[crawler] Crawling 2 URLs (max_concurrent=5)...
[crawler] Completed: 2/2 successful
https://react.dev: 15234 chars    # æ­£å¸¸å†…å®¹
https://vuejs.org: 12456 chars    # æ­£å¸¸å†…å®¹

=== å¯¹æ¯” ===
https://react.dev:
  åŸç‰ˆæœ¬: 0 chars
  ä¼˜åŒ–ç‰ˆæœ¬: 15234 chars
  æå‡: +15234%

https://vuejs.org:
  åŸç‰ˆæœ¬: 0 chars
  ä¼˜åŒ–ç‰ˆæœ¬: 12456 chars
  æå‡: +12456%
```

---

### æµ‹è¯•æ¡ˆä¾‹ 3: æ€§èƒ½å¯¹æ¯”æµ‹è¯•

**åˆ›å»ºæ€§èƒ½æµ‹è¯•è„šæœ¬**ï¼š

```python
# test_crawler_performance.py

import time
import asyncio
from tools.crawler import _crawl_urls_legacy, crawl_urls  # ä»åˆå¹¶åçš„ crawler.py å¯¼å…¥

def test_performance():
    """å¯¹æ¯”æµ‹è¯•ï¼šåŸç‰ˆæœ¬ vs ä¼˜åŒ–ç‰ˆæœ¬ï¼ˆæ€§èƒ½ï¼‰"""
    test_urls = [
        "https://www.baidu.com",
        "https://www.example.com",
        "https://www.python.org",
        "https://www.github.com",
        "https://www.stackoverflow.com",
    ]

    print("=== æ€§èƒ½å¯¹æ¯”æµ‹è¯• ===")
    print(f"æµ‹è¯• URL æ•°é‡: {len(test_urls)}\n")

    # æµ‹è¯•åŸç‰ˆæœ¬
    print("æµ‹è¯•åŸç‰ˆæœ¬ (urllib, é¡ºåº)...")
    start = time.time()
    results1 = _crawl_urls_legacy(test_urls)  # ç›´æ¥ä½¿ç”¨ legacy å‡½æ•°
    time1 = time.time() - start
    success1 = sum(1 for r in results1 if "failed" not in r["content"].lower())

    # æµ‹è¯•ä¼˜åŒ–ç‰ˆæœ¬
    print("æµ‹è¯•ä¼˜åŒ–ç‰ˆæœ¬ (Playwright, å¹¶å‘)...")
    start = time.time()
    results2 = crawl_urls(test_urls)  # ä¼šè‡ªåŠ¨ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬
    time2 = time.time() - start
    success2 = sum(1 for r in results2 if "failed" not in r["content"].lower())

    # ç»“æœå¯¹æ¯”
    print("\n=== ç»“æœ ===")
    print(f"åŸç‰ˆæœ¬:")
    print(f"  è€—æ—¶: {time1:.2f}s")
    print(f"  æˆåŠŸ: {success1}/{len(test_urls)}")
    print(f"\nä¼˜åŒ–ç‰ˆæœ¬:")
    print(f"  è€—æ—¶: {time2:.2f}s")
    print(f"  æˆåŠŸ: {success2}/{len(test_urls)}")
    print(f"\næ€§èƒ½æå‡: {(time1-time2)/time1*100:.1f}%")
    print(f"æé€Ÿå€æ•°: {time1/time2:.1f}x")

if __name__ == "__main__":
    test_performance()
```

**è¿è¡Œæµ‹è¯•**ï¼š
```bash
python test_crawler_performance.py
```

**é¢„æœŸè¾“å‡º**ï¼š
```
=== æ€§èƒ½å¯¹æ¯”æµ‹è¯• ===
æµ‹è¯• URL æ•°é‡: 5

æµ‹è¯•åŸç‰ˆæœ¬ (urllib, é¡ºåº)...
æµ‹è¯•ä¼˜åŒ–ç‰ˆæœ¬ (Playwright, å¹¶å‘)...
[crawler] Crawling 5 URLs (max_concurrent=5)...
[crawler] Completed: 5/5 successful

=== ç»“æœ ===
åŸç‰ˆæœ¬:
  è€—æ—¶: 15.23s
  æˆåŠŸ: 4/5

ä¼˜åŒ–ç‰ˆæœ¬:
  è€—æ—¶: 3.87s
  æˆåŠŸ: 5/5

æ€§èƒ½æå‡: 74.6%
æé€Ÿå€æ•°: 3.9x
```

---

### æµ‹è¯•æ¡ˆä¾‹ 4: é›†æˆæµ‹è¯•ï¼ˆDeepSearchï¼‰

**åœ¨ DeepSearch ä¸­æµ‹è¯•**ï¼š

```bash
# 1. ç¡®ä¿ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬
echo "USE_OPTIMIZED_CRAWLER=true" >> .env
echo "DEEPSEARCH_ENABLE_CRAWLER=true" >> .env

# 2. é‡å¯åº”ç”¨
python main.py

# 3. å‘é€è¯·æ±‚
curl -X POST http://localhost:8000/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [{"role": "user", "content": "2024å¹´äººå·¥æ™ºèƒ½æœ€æ–°è¿›å±•"}],
    "search_mode": "deep",
    "stream": false
  }'

# 4. æŸ¥çœ‹æ—¥å¿—
tail -f logs/weaver.log | grep -E "\[crawler\]|\[deepsearch\]"
```

**é¢„æœŸæ—¥å¿—**ï¼š
```
[deepsearch] Epoch 1: æœç´¢åˆ° 25 ä¸ªç»“æœ | ç´¯è®¡ URL: 25 | è€—æ—¶ 5.67s
[deepsearch] Epoch 1: é€‰æ‹© 5 ä¸ª URL | å·²é€‰æ€»æ•°: 5 | è€—æ—¶ 1.23s
[crawler] Using optimized Playwright-based crawler
[crawler] Crawling 5 URLs (max_concurrent=5)...
[crawler] âœ“ https://example1.com (15234 chars)
[crawler] âœ“ https://example2.com (12456 chars)
[crawler] Completed: 5/5 successful
[deepsearch] Epoch 1: çˆ¬è™«å¢å¼ºå®Œæˆ | è€—æ—¶ 3.45s
```

---

## ğŸ“ˆ ç›‘æ§å’Œè°ƒä¼˜

### å…³é”®æŒ‡æ ‡

1. **çˆ¬å–æˆåŠŸç‡**ï¼š`success_count / total_urls`
   - æœŸæœ›å€¼ï¼šâ‰¥ 90%
   - ä½äº 80% è¯´æ˜é…ç½®éœ€è¦è°ƒæ•´

2. **å¹³å‡è€—æ—¶**ï¼š`total_time / url_count`
   - æœŸæœ›å€¼ï¼šâ‰¤ 1s/URL
   - è¿‡é«˜è¯´æ˜è¶…æ—¶é…ç½®å¤ªé•¿æˆ–ç½‘ç»œé—®é¢˜

3. **å†…å®¹å®Œæ•´æ€§**ï¼š`avg(content_length)`
   - æœŸæœ›å€¼ï¼šâ‰¥ 1000 chars
   - è¿‡ä½è¯´æ˜çˆ¬å–ä¸å®Œæ•´

### é…ç½®è°ƒä¼˜å»ºè®®

**åœºæ™¯ 1: çˆ¬å–é€Ÿåº¦æ…¢**

```bash
# åŸé…ç½®
CRAWLER_MAX_CONCURRENT=3
CRAWLER_PAGE_TIMEOUT=20000
CRAWLER_WAIT_UNTIL=load

# ä¼˜åŒ–é…ç½®
CRAWLER_MAX_CONCURRENT=8           # å¢åŠ å¹¶å‘
CRAWLER_PAGE_TIMEOUT=15000         # å‡å°‘è¶…æ—¶
CRAWLER_WAIT_UNTIL=domcontentloaded # æ›´å¿«çš„ç­‰å¾…ç­–ç•¥
```

**åœºæ™¯ 2: çˆ¬å–å¤±è´¥ç‡é«˜**

```bash
# åŸé…ç½®
CRAWLER_MAX_CONCURRENT=10
CRAWLER_PAGE_TIMEOUT=10000

# ä¼˜åŒ–é…ç½®
CRAWLER_MAX_CONCURRENT=3           # é™ä½å¹¶å‘
CRAWLER_PAGE_TIMEOUT=30000         # å¢åŠ è¶…æ—¶
```

**åœºæ™¯ 3: å†…å®¹ä¸å®Œæ•´**

```bash
# åŸé…ç½®
CRAWLER_WAIT_UNTIL=domcontentloaded

# ä¼˜åŒ–é…ç½®
CRAWLER_WAIT_UNTIL=networkidle     # ç­‰å¾…ç½‘ç»œç©ºé—²
```

---

## ğŸ”§ å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•å®‰è£… Playwrightï¼Ÿ

**é—®é¢˜**ï¼š`ModuleNotFoundError: No module named 'playwright'`

**è§£å†³**ï¼š
```bash
# Step 1: å®‰è£… Python åŒ…
pip install playwright

# Step 2: å®‰è£…æµè§ˆå™¨
playwright install chromium

# Step 3: éªŒè¯å®‰è£…
python -c "from playwright.async_api import async_playwright; print('OK')"
```

---

### Q2: ä¼˜åŒ–ç‰ˆæœ¬æ¯”åŸç‰ˆæœ¬æ…¢ï¼Ÿ

**å¯èƒ½åŸå› **ï¼š
1. æµè§ˆå™¨å¯åŠ¨æœ‰åˆå§‹åŒ–å¼€é”€ï¼ˆçº¦ 1-2sï¼‰
2. URL æ•°é‡å¤ªå°‘ï¼ˆ< 3 ä¸ªï¼‰

**è§£å†³æ–¹æ¡ˆ**ï¼š

**æ–¹æ¡ˆ 1: ä½¿ç”¨å…¨å±€å•ä¾‹**
```python
from tools.crawler_optimized import get_global_crawler

async def my_function():
    crawler = await get_global_crawler()  # å¤ç”¨æµè§ˆå™¨
    results = await crawler.crawl_urls(urls)
```

**åœºæ™¯ 2: æ ¹æ® URL æ•°é‡é€‰æ‹©**
```python
def smart_crawl(urls):
    from tools.crawler import _crawl_urls_legacy, crawl_urls

    if len(urls) < 3:
        # URL å¤ªå°‘ï¼Œç”¨ urllib æ›´å¿«ï¼ˆé¿å…æµè§ˆå™¨å¯åŠ¨å¼€é”€ï¼‰
        return _crawl_urls_legacy(urls)
    else:
        # URL è¾ƒå¤šï¼Œç”¨ Playwright æ›´å¿«
        return crawl_urls(urls)
```

---

### Q3: å¦‚ä½•è°ƒè¯•çˆ¬è™«ï¼Ÿ

**æ–¹æ³• 1: éæ— å¤´æ¨¡å¼ï¼ˆæŸ¥çœ‹æµè§ˆå™¨ï¼‰**

```bash
# .env
CRAWLER_HEADLESS=false
```

**æ–¹æ³• 2: å¯ç”¨è¯¦ç»†æ—¥å¿—**

```python
import logging
logging.getLogger("crawler").setLevel(logging.DEBUG)
```

**æ–¹æ³• 3: æˆªå›¾è°ƒè¯•**

```python
# åœ¨ crawler_optimized.py çš„ crawl_single_url ä¸­æ·»åŠ 
await page.screenshot(path=f"debug_{url.replace('/', '_')}.png")
```

---

### Q4: çˆ¬è™«ç»å¸¸è¶…æ—¶æ€ä¹ˆåŠï¼Ÿ

**åŸå› åˆ†æ**ï¼š
1. ç½‘ç»œæ…¢
2. ç½‘ç«™åŠ è½½æ…¢
3. è¶…æ—¶é…ç½®å¤ªçŸ­

**è§£å†³æ–¹æ¡ˆ**ï¼š

```bash
# .env
CRAWLER_PAGE_TIMEOUT=30000  # å¢åŠ åˆ° 30 ç§’
CRAWLER_WAIT_UNTIL=domcontentloaded  # ä¸ç­‰å¾…æ‰€æœ‰èµ„æºåŠ è½½
```

---

### Q5: å¦‚ä½•åœ¨ç”Ÿäº§ç¯å¢ƒä½¿ç”¨ï¼Ÿ

**æ¨èé…ç½®**ï¼š

```bash
# .env (ç”Ÿäº§ç¯å¢ƒ)
USE_OPTIMIZED_CRAWLER=true
CRAWLER_HEADLESS=true          # æ— å¤´æ¨¡å¼èŠ‚çœèµ„æº
CRAWLER_PAGE_TIMEOUT=20000     # å¹³è¡¡é€Ÿåº¦å’ŒæˆåŠŸç‡
CRAWLER_MAX_CONCURRENT=5       # é€‚ä¸­å¹¶å‘
CRAWLER_WAIT_UNTIL=domcontentloaded
```

**ç›‘æ§**ï¼š
```python
# æ·»åŠ  Prometheus ç›‘æ§
from prometheus_client import Histogram

crawler_duration = Histogram(
    'crawler_duration_seconds',
    'Crawler execution duration'
)
```

---

## ğŸ“ æœ€ä½³å®è·µ

### 1. å¼€å‘ç¯å¢ƒ vs ç”Ÿäº§ç¯å¢ƒ

**å¼€å‘ç¯å¢ƒ**ï¼š
```bash
# .env.development
CRAWLER_HEADLESS=false        # æŸ¥çœ‹æµè§ˆå™¨
CRAWLER_PAGE_TIMEOUT=30000    # é•¿è¶…æ—¶
CRAWLER_MAX_CONCURRENT=3      # ä½å¹¶å‘
LOG_LEVEL=DEBUG               # è¯¦ç»†æ—¥å¿—
```

**ç”Ÿäº§ç¯å¢ƒ**ï¼š
```bash
# .env.production
CRAWLER_HEADLESS=true         # æ— å¤´æ¨¡å¼
CRAWLER_PAGE_TIMEOUT=20000    # ä¸­ç­‰è¶…æ—¶
CRAWLER_MAX_CONCURRENT=5      # ä¸­ç­‰å¹¶å‘
LOG_LEVEL=INFO                # ç®€æ´æ—¥å¿—
```

---

### 2. é™çº§ç­–ç•¥

```python
# tools/crawler.py

def crawl_urls(urls: List[str]) -> List[Dict[str, Any]]:
    """æ™ºèƒ½é™çº§çš„ crawlerã€‚"""
    # ä¼˜å…ˆä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬
    if settings.use_optimized_crawler:
        try:
            from tools.crawler_optimized import crawl_urls as crawl_opt
            return crawl_opt(urls)
        except ImportError:
            logger.warning("Playwright not available, using fallback")
        except Exception as e:
            logger.error(f"Optimized crawler failed: {e}, using fallback")

    # é™çº§åˆ° urllib
    return _crawl_urls_legacy(urls)
```

---

### 3. æµè§ˆå™¨å¤ç”¨

```python
# å…¨å±€å•ä¾‹æ¨¡å¼ï¼ˆæ¨èï¼‰
from tools.crawler_optimized import get_global_crawler, close_global_crawler

async def application_startup():
    """åº”ç”¨å¯åŠ¨æ—¶åˆå§‹åŒ– crawler"""
    await get_global_crawler()

async def application_shutdown():
    """åº”ç”¨å…³é—­æ—¶æ¸…ç† crawler"""
    await close_global_crawler()

# åœ¨ FastAPI ä¸­ä½¿ç”¨
@app.on_event("startup")
async def startup():
    await application_startup()

@app.on_event("shutdown")
async def shutdown():
    await application_shutdown()
```

---

## ğŸ¯ åç»­ä¼˜åŒ–æ–¹å‘

1. **æ™ºèƒ½é‡è¯•** - è¶…æ—¶æ—¶è‡ªåŠ¨é‡è¯•ï¼ŒæŒ‡æ•°é€€é¿
2. **ç¼“å­˜æœºåˆ¶** - ç¼“å­˜å·²çˆ¬å–çš„ URL å†…å®¹
3. **ä»£ç†æ”¯æŒ** - æ”¯æŒ HTTP/SOCKS ä»£ç†
4. **åçˆ¬è™«å¯¹æŠ—** - Stealth æ¨¡å¼ã€éšæœº UA
5. **å†…å®¹æå–** - æ™ºèƒ½æå–æ­£æ–‡ï¼ˆReadabilityï¼‰
6. **æˆªå›¾æ”¯æŒ** - ä¿å­˜é¡µé¢æˆªå›¾ç”¨äºè°ƒè¯•

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [CRAWLER_OPTIMIZATION.md](./CRAWLER_OPTIMIZATION.md) - Crawler ä¼˜åŒ–æ–¹æ¡ˆ
- [DEEPSEARCH_OPTIMIZATION.md](./DEEPSEARCH_OPTIMIZATION.md) - DeepSearch ä¼˜åŒ–æ–¹æ¡ˆ
- [DEEPSEARCH_USAGE.md](./DEEPSEARCH_USAGE.md) - DeepSearch ä½¿ç”¨æŒ‡å—

---

## ğŸ¤ åé¦ˆå’Œæ”¹è¿›

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·ï¼š
1. æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶ï¼š`logs/weaver.log`
2. è¿è¡Œæµ‹è¯•è„šæœ¬éªŒè¯
3. æäº¤ Issue æˆ– PR

---

**ç‰ˆæœ¬**: v1.0.0
**æœ€åæ›´æ–°**: 2025-12-20
**ä½œè€…**: Weaver Team
